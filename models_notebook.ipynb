{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative analyses of machine learning-based models\n",
    "This juptyer notebook goes through the compiling, training, testing and visualisation of machine learning models used for bacterial taxonomy classification using the curated 16S rRNA sequence datasets.\n",
    "\n",
    "## Loading modules and training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded!\n"
     ]
    }
   ],
   "source": [
    "# LOADING PACKAGES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from varname import argname\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, accuracy_score\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Masking, Dot, Add, BatchNormalization\n",
    "from keras.layers import MaxPooling1D, AveragePooling1D, Conv1D, Reshape\n",
    "from keras.layers import TimeDistributed, LSTM, Bidirectional\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "LR, BATCH_SIZE, EPOCHS, = 0.001, 128, 60\n",
    "MAX_LEN, MAX_LEN_V, INPUT_SHAPE_RNN, INPUT_SHAPE_RNN_V, = 2000, 500, (2000, 4), (500, 4)    # cut-off length\n",
    "INPUT_SHAPE_3_MER, INPUT_SHAPE_5_MER, INPUT_SHAPE_7_MER = (5**3, 1), (5**5, 1), (5**7, 1)   # n**k\n",
    "\n",
    "print('Packages loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded!\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE NON-AUGMENTED AND AUGMENTED DATASETS\n",
    "train_na = pd.read_csv('df_train_0.csv')\n",
    "val_na = pd.read_csv('df_val_0.csv')\n",
    "test_na = pd.read_csv('df_test_0.csv')\n",
    "# ------------------------------------------------\n",
    "train_a = pd.read_csv('df_train_1.csv')\n",
    "val_a = pd.read_csv('df_val_1.csv')\n",
    "# ------------------------------------------------\n",
    "print('Datasets loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating k-mers\n",
    "For use in the Convolutional Neural Networks (CNN), the sequences are processed into a frequency table of k-mers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# define all possible k-mers\n",
    "alphabet = \"AGTCN\"\n",
    "tri_mers = [''.join(chars) for chars in product(*(3*(alphabet,)))]\n",
    "fiv_mers = [''.join(chars) for chars in product(*(5*(alphabet,)))]\n",
    "sev_mers = [''.join(chars) for chars in product(*(7*(alphabet,)))]\n",
    "\n",
    "def one_hot_k(sequence, kmers):\n",
    "    k = len(kmers[0])\n",
    "    # define a counter dictionary\n",
    "    kmer_dict = dict.fromkeys(kmers, 0)\n",
    "\n",
    "    # standardize the sequence\n",
    "    # by replacing U with T and all ambiguous bases with N\n",
    "    sequence = sequence.replace('U', 'T').replace('Y', 'N').replace('R', 'N').replace('W', 'N').replace('S', 'N').replace('K', 'N').replace('M', 'N').replace('D', 'N').replace('V', 'N').replace('H', 'N').replace('B', 'N').replace('X', 'N').replace('-', 'N')\n",
    "    # count every k-mer in the sequence\n",
    "    for i in range(0, len(sequence) - k+1):\n",
    "        kmer_dict[sequence[i:i+k]] += 1\n",
    "\n",
    "    # k-mer frequency array from the dictionary values\n",
    "    k_array = np.array(list(kmer_dict.values()))\n",
    "    # normalizing the array by dividing every value with the highest count value\n",
    "    k_array = k_array / np.amax(k_array)\n",
    "    return k_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding the sequences\n",
    "For use in the various Recurrent Neural Networks (RNN), the nucleotide sequences are processed into a one-hot-encoded format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary without consideration of mutation rate\n",
    "one_hot_dict0 = {\n",
    "    'A': [1.,0.,0.,0.], 'G':[0.,1.,0.,0.], 'T':[0.,0.,1.,0.], 'U':[0.,0.,1.,0.], 'C':[0.,0.,0.,1.], \n",
    "    'Y':[0.,0.,0.5,0.5], 'R':[0.5,0.5,0.,0.], 'W':[0.5,0.,0.5,0.], 'S':[0.,0.5,0.,0.5], 'K':[0.,0.5,0.5,0.], 'M':[0.5,0.,0.,0.5], \n",
    "    'D':[0.33,0.33,0.33,0.], 'V':[0.33,0.33,0.,0.33], 'H':[0.33,0.,0.33,0.33], 'B':[0.,0.33,0.33,0.33], \n",
    "    'X':[0.25,0.25,0.25,0.25], 'N':[0.25,0.25,0.25,0.25], '-':[0.,0.,0.,0.]\n",
    "    }\n",
    "# Dictionary with consideration of mutation rate (transition >> transversion)\n",
    "one_hot_dict1 = {\n",
    "    'A': [1.,0.,-0.5,-0.5], 'G':[0.,1.,-0.5,-0.5], 'T':[-0.5,-0.5,1.,0.], 'U':[-0.5,-0.5,1.,0.], 'C':[-0.5,-0.5,0.,1.], \n",
    "    'Y':[-0.5,-0.5,0.5,0.5], 'R':[0.5,0.5,-0.5,-0.5], 'W':[0.5,-0.5,0.5,-0.5], 'S':[-0.5,0.5,-0.5,0.5], 'K':[-0.5,0.5,0.5,-0.5], 'M':[0.5,-0.5,-0.5,0.5], \n",
    "    'D':[0.33,0.33,0.33,-1.], 'V':[0.33,0.33,-1.,0.33], 'H':[0.33,-1.,.33,0.33], 'B':[-1.,0.33,0.33,0.33], \n",
    "    'X':[0.,0.,0.,0.], 'N':[0.,0.,0.,0.], '-':[0.,0.,0.,0.]\n",
    "    }\n",
    "\n",
    "def one_hot_seq(sequence, one_hot_dict, max_len):\n",
    "    # padding the sequences to a fixed length\n",
    "\tsequence += '-'*(max_len - len(sequence))\n",
    "    # generating list of one-hot-lists using the dictionary\n",
    "\tonehot_encoded = [one_hot_dict[nucleotide] for nucleotide in sequence]\n",
    "    # returning the list of lists as a numpy array\n",
    "\treturn np.array(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding the labels\n",
    "For use in the deep learning models, the labels are processed into a one-hot-encoding format. To achieve this, every unique label is first encoded to a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxon_dict(df, taxon):\n",
    "    # listing all unique taxon labels\n",
    "    taxon_list = list(df[taxon].unique())\n",
    "\n",
    "    # generating a dictionary to associate every unique taxon to a number\n",
    "    taxon_dict = dict(zip(taxon_list, range(0, len(taxon_list))))\n",
    "    # and the reversed dictionary as a lookup table\n",
    "    taxon_dict_lookup = {v: k for k, v in taxon_dict.items()}\n",
    "\n",
    "    return taxon_dict, taxon_dict_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the input sequences and labels\n",
    "The encoding methods for nucleotide sequences (x) defined earlier are now applied. The labels (y) are one-hot-encoded using the to_categorical function in keras_utils, thereby, converting the data into the correct format for feeding it to the deep learning models.\n",
    "\n",
    "Every array is saved to reduce memory requirements down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the sequences into k-mer counts and one-hot-encoded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-mer complete\n",
      "6-mer complete\n",
      "CNN sequences complete\n"
     ]
    }
   ],
   "source": [
    "# ENCODING SEQUENCES for the CNN models in 2 processing variations\n",
    "# FOR CNN  |  with 3-mer encoding\n",
    "x_train_CNN_na3 = np.array(\n",
    "    train_na['Sequence'].apply(lambda x: one_hot_k(x, tri_mers)).tolist())\n",
    "np.save('arrays/CNN/x_train_CNN_na3.npy', x_train_CNN_na3)\n",
    "\n",
    "# x_train_CNN_a3 = np.array(\n",
    "#   train_a['Sequence'].apply(lambda x: one_hot_k(x, tri_mers)).tolist())\n",
    "# np.save('arrays/CNN/x_train_CNN_a3.npy', x_train_CNN_a3)\n",
    "\n",
    "x_test_CNN_na3 = np.array(\n",
    "    test_na['Sequence'].apply(lambda x: one_hot_k(x, tri_mers)).tolist())\n",
    "np.save('arrays/CNN/x_test_CNN_na3.npy', x_test_CNN_na3)\n",
    "\n",
    "dataval_CNN_na3 = np.array(\n",
    "    val_na['Sequence'].apply(lambda x: one_hot_k(x, tri_mers)).tolist())\n",
    "np.save('arrays/CNN/dataval_CNN_na3.npy', dataval_CNN_na3)\n",
    "\n",
    "# dataval_CNN_a3 = np.array(\n",
    "#   val_a['Sequence'].apply(lambda x: one_hot_k(x, tri_mers)).tolist())\n",
    "# np.save('arrays/CNN/dataval_CNN_a3.npy', dataval_CNN_a3)\n",
    "print('3-mer encoding complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# FOR CNN  |  with 5-mer encoding\n",
    "x_train_CNN_na5 = np.array(\n",
    "    train_na['Sequence'].apply(lambda x: one_hot_k(x, fiv_mers)).tolist())\n",
    "np.save('arrays/CNN/x_train_CNN_na5.npy', x_train_CNN_na5)\n",
    "\n",
    "# x_train_CNN_a5 = np.array(\n",
    "#   train_a['Sequence'].apply(lambda x: one_hot_k(x, fiv_mers)).tolist())\n",
    "# np.save('arrays/CNN/x_train_CNN_a5.npy', x_train_CNN_a5)\n",
    "\n",
    "x_test_CNN_na5 = np.array(\n",
    "    test_na['Sequence'].apply(lambda x: one_hot_k(x, fiv_mers)).tolist())\n",
    "np.save('arrays/CNN/x_test_CNN_na5.npy', x_test_CNN_na5)\n",
    "\n",
    "dataval_CNN_na5 = np.array(\n",
    "    val_na['Sequence'].apply(lambda x: one_hot_k(x, fiv_mers)).tolist())\n",
    "np.save('arrays/CNN/dataval_CNN_na5.npy', dataval_CNN_na5)\n",
    "\n",
    "# dataval_CNN_a5 = np.array(\n",
    "#   val_a['Sequence'].apply(lambda x: one_hot_k(x, fiv_mers)).tolist())\n",
    "# np.save('arrays/CNN/dataval_CNN_a5.npy', dataval_CNN_a5)\n",
    "print('5-mer encoding complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# FOR CNN  |  with 7-mer encoding\n",
    "x_train_CNN_na7 = np.array(\n",
    "    train_na['Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "np.save('arrays/CNN/x_train_CNN_na7.npy', x_train_CNN_na7)\n",
    "\n",
    "x_train_CNN_a7 = np.array(\n",
    "    train_a['Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "np.save('arrays/CNN/x_train_CNN_a7.npy', x_train_CNN_a7)\n",
    "\n",
    "x_test_CNN_na7 = np.array(\n",
    "    test_na['Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "np.save('arrays/CNN/x_test_CNN_na7.npy', x_test_CNN_na7)\n",
    "\n",
    "dataval_CNN_na7 = np.array(\n",
    "    val_na['Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "np.save('arrays/CNN/dataval_CNN_na7.npy', dataval_CNN_na7)\n",
    "\n",
    "dataval_CNN_a7 = np.array(\n",
    "    val_a['Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "np.save('arrays/CNN/dataval_CNN_a7.npy', dataval_CNN_a7)\n",
    "print('7-mer encoding complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# FOR CNN  |  with 7-mer encoding on V-region selected sequences\n",
    "x_train_CNN_v7 = np.array(\n",
    "    train_na['V_Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "np.save('arrays/CNN/x_train_CNN_v7.npy', x_train_CNN_v7)\n",
    "\n",
    "# x_train_CNN_va7 = np.array(\n",
    "#     train_a['V_Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "# np.save('arrays/CNN/x_train_CNN_va7.npy', x_train_CNN_va7)\n",
    "\n",
    "x_test_CNN_v7 = np.array(\n",
    "    test_na['V_Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "np.save('arrays/CNN/x_test_CNN_v7.npy', x_test_CNN_v7)\n",
    "\n",
    "dataval_CNN_v7 = np.array(\n",
    "    val_na['V_Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "np.save('arrays/CNN/dataval_CNN_v7.npy', dataval_CNN_v7)\n",
    "\n",
    "# dataval_CNN_va7 = np.array(\n",
    "#     val_a['V_Sequence'].apply(lambda x: one_hot_k(x, sev_mers)).tolist())\n",
    "# np.save('arrays/CNN/dataval_CNN_va7.npy', dataval_CNN_va7)\n",
    "print('7-mer encoding complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "print('CNN sequences complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular one-hot-encoding complete\n",
      "Mutations rate adjusted one-hot-encoding complete\n",
      "RNN sequences complete\n"
     ]
    }
   ],
   "source": [
    "# ENCODING SEQUENCES for the RNN models in 2 processing variations\n",
    "# FOR RNN  |  with regular one-hot-encoding\n",
    "x_train_RNN_na0 = np.array(\n",
    "    train_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0, max_len = MAX_LEN)).tolist())\n",
    "np.save('arrays/RNN/x_train_RNN_na0.npy', x_train_RNN_na0)\n",
    "\n",
    "# x_train_RNN_a0 = np.array(\n",
    "#   train_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0, max_len = MAX_LEN)).tolist())\n",
    "# np.save('arrays/RNN/x_train_RNN_a0.npy', x_train_RNN_a0)\n",
    "\n",
    "x_test_RNN_na0 = np.array(\n",
    "    test_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0, max_len = MAX_LEN)).tolist())\n",
    "np.save('arrays/RNN/x_test_RNN_na0.npy', x_test_RNN_na0)\n",
    "\n",
    "dataval_RNN_na0 = np.array(\n",
    "    val_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0, max_len = MAX_LEN)).tolist())\n",
    "np.save('arrays/RNN/dataval_RNN_na0.npy', dataval_RNN_na0)\n",
    "\n",
    "# dataval_RNN_a0 = np.array(\n",
    "#   val_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0, max_len = MAX_LEN)).tolist())\n",
    "# np.save('arrays/RNN/dataval_RNN_a0.npy', dataval_RNN_a0)\n",
    "print('Regular one-hot-encoding complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# FOR RNN  |  with matation rate adjusted one-hot-encoding\n",
    "x_train_RNN_na1 = np.array(\n",
    "    train_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN)).tolist())\n",
    "np.save('arrays/RNN/x_train_RNN_na1.npy', x_train_RNN_na1)\n",
    "\n",
    "x_train_RNN_a1 = np.array(\n",
    "    train_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN)).tolist())\n",
    "np.save('arrays/RNN/x_train_RNN_a1.npy', x_train_RNN_a1)\n",
    "\n",
    "x_test_RNN_na1 = np.array(\n",
    "    test_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN)).tolist())\n",
    "np.save('arrays/RNN/x_test_RNN_na1.npy', x_test_RNN_na1)\n",
    "\n",
    "dataval_RNN_na1 = np.array(\n",
    "    val_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN)).tolist())\n",
    "np.save('arrays/RNN/dataval_RNN_na1.npy', dataval_RNN_na1)\n",
    "\n",
    "dataval_RNN_a1 = np.array(\n",
    "    val_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN)).tolist())\n",
    "np.save('arrays/RNN/dataval_RNN_a1.npy', dataval_RNN_a1)\n",
    "print('Mutations rate adjusted one-hot-encoding complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# FOR RNN  |  with matation rate adjusted one-hot-encoding on the V-region selected sequences\n",
    "x_train_RNN_v1 = np.array(\n",
    "    train_na['V_Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN_V)).tolist())\n",
    "np.save('arrays/RNN/x_train_RNN_na1.npy', x_train_RNN_na1)\n",
    "\n",
    "# x_train_RNN_va1 = np.array(\n",
    "#   train_a['V_Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN_V)).tolist())\n",
    "# np.save('arrays/RNN/x_train_RNN_a1.npy', x_train_RNN_a1)\n",
    "\n",
    "x_test_RNN_v1 = np.array(\n",
    "    test_na['V_Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN_V)).tolist())\n",
    "np.save('arrays/RNN/x_test_RNN_na1.npy', x_test_RNN_na1)\n",
    "\n",
    "dataval_RNN_v1 = np.array(\n",
    "    val_na['V_Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN_V)).tolist())\n",
    "np.save('arrays/RNN/dataval_RNN_na1.npy', dataval_RNN_na1)\n",
    "\n",
    "# dataval_RNN_va1 = np.array(\n",
    "#   val_a['V_Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1, max_len = MAX_LEN_V)).tolist())\n",
    "# np.save('arrays/RNN/dataval_RNN_a1.npy', dataval_RNN_a1)\n",
    "print('Mutations rate adjusted one-hot-encoding V-region complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "print('RNN sequences complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels into a one-hot-encoded format at Family level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels complete\n",
      "Family train/test/val arrays generated\n",
      "The number of unique family labels: 349\n"
     ]
    }
   ],
   "source": [
    "taxon = 'Family'\n",
    "taxon_dict = get_taxon_dict(test_na, taxon)[0]\n",
    "# -----------------------------------------------------------------------------\n",
    "# Associate every entry's label in the df to a number using the dictionary \n",
    "#   & one-hot encode the numerical labels\n",
    "#   & save the result as a numpy array\n",
    "y_train_fam_na = to_categorical(\n",
    "    y = train_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/family/y_train_fam_na.npy', y_train_fam_na)\n",
    "\n",
    "# y_train_fam_a = to_categorical(\n",
    "#   y = train_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "# np.save('arrays/family/y_train_fam_a.npy', y_train_fam_a)\n",
    "\n",
    "y_test_fam_na = to_categorical(\n",
    "    y = test_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/family/y_test_fam_na.npy', y_test_fam_na)\n",
    "\n",
    "labelsval_fam_na = to_categorical(\n",
    "    y = val_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/family/labelsval_fam_na.npy', labelsval_fam_na)\n",
    "\n",
    "# labelsval_fam_a = to_categorical(\n",
    "#   y = val_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "# np.save('arrays/family/labelsval_fam_a.npy', labelsval_fam_a)\n",
    "print('Family label arrays generated')\n",
    "# -----------------------------------------------------------------------------\n",
    "fam_count = train_na[taxon].nunique()\n",
    "print(f'The number of unique family labels: {fam_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels into a one-hot-encoded format at Genus level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genus label arrays generated\n",
      "The number of unique genus labels: 954\n"
     ]
    }
   ],
   "source": [
    "taxon = 'Genus'\n",
    "taxon_dict = get_taxon_dict(test_na, taxon)[0]\n",
    "# -----------------------------------------------------------------------------\n",
    "y_train_gen_na = to_categorical(\n",
    "    y = train_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/genus/y_train_gen_na.npy', y_train_gen_na)\n",
    "\n",
    "y_train_gen_a = to_categorical(\n",
    "    y = train_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/genus/y_train_gen_a.npy', y_train_gen_a)\n",
    "\n",
    "y_test_gen_na = to_categorical(\n",
    "    y = test_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/genus/y_test_gen_na.npy', y_test_gen_na)\n",
    "\n",
    "labelsval_gen_na = to_categorical(\n",
    "    y = val_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/genus/labelsval_gen_na.npy', labelsval_gen_na)\n",
    "\n",
    "labelsval_gen_a = to_categorical(\n",
    "    y = val_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/genus/labelsval_gen_a.npy', labelsval_gen_a)\n",
    "print('Genus label arrays generated')\n",
    "# -----------------------------------------------------------------------------\n",
    "gen_count = train_na[taxon].nunique()\n",
    "print(f'The number of unique genus labels: {gen_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels into a one-hot-encoded format at Species level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species train/test/val arrays generated\n",
      "The number of unique species labels: 1569\n"
     ]
    }
   ],
   "source": [
    "taxon = 'Species'\n",
    "taxon_dict = get_taxon_dict(test_na, taxon)[0]\n",
    "# -----------------------------------------------------------------------------\n",
    "y_train_spe_na = to_categorical(\n",
    "    y = train_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/species/y_train_spe_na.npy', y_train_spe_na)\n",
    "\n",
    "# y_train_spe_a = to_categorical(\n",
    "#   y = train_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "# np.save('arrays/species/y_train_spe_a.npy', y_train_spe_a)\n",
    "\n",
    "y_test_spe_na = to_categorical(\n",
    "    y = test_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/species/y_test_spe_na.npy', y_test_spe_na)\n",
    "\n",
    "labelsval_spe_na = to_categorical(\n",
    "    y = val_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save('arrays/species/labelsval_spe_na.npy', labelsval_spe_na)\n",
    "\n",
    "# labelsval_spe_a = to_categorical(\n",
    "#   y = val_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "# np.save('arrays/species/labelsval_spe_a.npy', labelsval_spe_a)\n",
    "print('Species label arrays generated')\n",
    "# -----------------------------------------------------------------------------\n",
    "spe_count = train_na[taxon].nunique()\n",
    "print(f'The number of unique species labels: {spe_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the generated training/validation/testing arrays\n",
    "When seperating the processing and training/evaluation scripts, due to memory limits, the saved arrays can be loaded using this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMOUNT OF UNIQUE LABELS AT EACH TAXON LEVEL\n",
    "fam_count, gen_count, spe_count = 349, 954, 1569\n",
    "\n",
    "# LOADING ENCODED SEQUENCES for the CNN models in 2 processing variations\n",
    "# FOR CNN  |  with 3-mer\n",
    "x_train_CNN_na3 = np.load('arrays/CNN/x_train_CNN_na3.npy')\n",
    "# x_train_CNN_a3 = np.load('arrays/CNN/x_train_CNN_a3.npy')\n",
    "x_test_CNN_na3 = np.load('arrays/CNN/x_test_CNN_na3.npy')\n",
    "dataval_CNN_na3 = np.load('arrays/CNN/dataval_CNN_na3.npy')\n",
    "# dataval_CNN_a3 = np.load('arrays/CNN/dataval_CNN_a3.npy')\n",
    "print('3-mer encoded sequences LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# FOR CNN  |  with 5-mer\n",
    "x_train_CNN_na5 = np.load('arrays/CNN/x_train_CNN_na5.npy')\n",
    "# x_train_CNN_a5 = np.load('arrays/CNN/x_train_CNN_a5.npy')\n",
    "x_test_CNN_na5 = np.load('arrays/CNN/x_test_CNN_na5.npy')\n",
    "dataval_CNN_na5 = np.load('arrays/CNN/dataval_CNN_na5.npy')\n",
    "# dataval_CNN_a5 = np.load('arrays/CNN/dataval_CNN_a5.npy')\n",
    "print('5-mer encoded sequences LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# FOR CNN  |  with 7-mer\n",
    "x_train_CNN_na7 = np.load('arrays/CNN/x_train_CNN_na7.npy')\n",
    "x_train_CNN_a7 = np.load('arrays/CNN/x_train_CNN_a7.npy')\n",
    "x_test_CNN_na7 = np.load('arrays/CNN/x_test_CNN_na7.npy')\n",
    "dataval_CNN_na7 = np.load('arrays/CNN/dataval_CNN_na7.npy')\n",
    "dataval_CNN_a7 = np.load('arrays/CNN/dataval_CNN_a7.npy')\n",
    "print('7-mer encoded sequences LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# FOR CNN  |  with 7-mer on V-region selected data\n",
    "x_train_CNN_v7 = np.load('arrays/CNN/x_train_CNN_v7.npy')\n",
    "# x_train_CNN_va7 = np.load('arrays/CNN/x_train_CNN_va7.npy')\n",
    "x_test_CNN_v7 = np.load('arrays/CNN/x_test_CNN_v7.npy')\n",
    "dataval_CNN_v7 = np.load('arrays/CNN/dataval_CNN_v7.npy')\n",
    "# dataval_CNN_va7 = np.load('arrays/CNN/dataval_CNN_va7.npy')\n",
    "print('7-mer encoded V-region sequences LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "print('CNN sequences LOADED')\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# LOADING ENCODED SEQUENCES for the RNN models in 2 processing variations\n",
    "# FOR RNN  |  with regular one-hot-encoding\n",
    "x_train_RNN_na0 = np.load('arrays/RNN/x_train_RNN_na0.npy')\n",
    "# x_train_RNN_a0 = np.load('arrays/RNN/x_train_RNN_a0.npy')\n",
    "x_test_RNN_na0 = np.load('arrays/RNN/x_test_RNN_na0.npy')\n",
    "dataval_RNN_na0 = np.load('arrays/RNN/dataval_RNN_na0.npy')\n",
    "# dataval_RNN_a0 = np.load('arrays/RNN/dataval_RNN_a0.npy')\n",
    "print('Regular one-hot-encoded sequences LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# FOR RNN  |  with matation rate adjusted one-hot-encoding\n",
    "x_train_RNN_na1 = np.load('arrays/RNN/x_train_RNN_na1.npy')\n",
    "x_train_RNN_a1 = np.load('arrays/RNN/x_train_RNN_a1.npy')\n",
    "x_test_RNN_na1 = np.load('arrays/RNN/x_test_RNN_na1.npy')\n",
    "dataval_RNN_na1 = np.load('arrays/RNN/dataval_RNN_na1.npy')\n",
    "dataval_RNN_a1 = np.load('arrays/RNN/dataval_RNN_a1.npy')\n",
    "print('Mutation rate adjusted one-hot-encoded sequences LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# FOR RNN  |  with matation rate adjusted encoding on V-region selected data\n",
    "x_train_RNN_v1 = np.load('arrays/RNN/x_train_RNN_v1.npy')\n",
    "# x_train_RNN_va1 = np.load('arrays/RNN/x_train_RNN_va1.npy')\n",
    "x_test_RNN_v1 = np.load('arrays/RNN/x_test_RNN_v1.npy')\n",
    "dataval_RNN_v1 = np.load('arrays/RNN/dataval_RNN_v1.npy')\n",
    "# dataval_RNN_va1 = np.load('arrays/RNN/dataval_RNN_va1.npy')\n",
    "print('Mutation rate adjusted one-hot-encoded V-region sequences LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "print('RNN sequences LOADED')\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# LOADING one-hot encoded labels at each taxon level\n",
    "# -----------------------------------------------------------------------------\n",
    "# LABELS AT FAMILY LEVEL\n",
    "y_train_fam_na = np.load('arrays/family/y_train_fam_na.npy')\n",
    "# y_train_fam_a = np.load('arrays/family/y_train_fam_a.npy')\n",
    "y_test_fam_na = np.load('arrays/family/y_test_fam_na.npy')\n",
    "labelsval_fam_na = np.load('arrays/family/labelsval_fam_na.npy')\n",
    "# labelsval_fam_a = np.load('arrays/family/labelsval_fam_a.npy')\n",
    "print('Family label arrays LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# LABELS AT GENUS LEVEL\n",
    "y_train_gen_na = np.load('arrays/genus/y_train_gen_na.npy')\n",
    "y_train_gen_a = np.load('arrays/genus/y_train_gen_a.npy')\n",
    "y_test_gen_na = np.load('arrays/genus/y_test_gen_na.npy')\n",
    "labelsval_gen_na = np.load('arrays/genus/labelsval_gen_na.npy')\n",
    "labelsval_gen_a = np.load('arrays/genus/labelsval_gen_a.npy')\n",
    "print('Genus label arrays LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# LABELS AT SPECIES LEVEL\n",
    "y_train_spe_na = np.load('arrays/species/y_train_spe_na.npy')\n",
    "# y_train_spe_a = np.load('arrays/species/y_train_spe_a.npy')\n",
    "y_test_spe_na = np.load('arrays/species/y_test_spe_na.npy')\n",
    "labelsval_spe_na = np.load('arrays/species/labelsval_spe_na.npy')\n",
    "# labelsval_spe_a = np.load('arrays/species/labelsval_spe_a.npy')\n",
    "print('Species label arrays LOADED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the network architectures\n",
    "What follows are a set of functions for creating the deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "def make_CNNmodel(input_shape, out_len, name = 'CNN'):\n",
    "    CNNmodel = keras.Sequential(\n",
    "        [\n",
    "            Reshape(target_shape = input_shape, input_shape = input_shape[:-1]),\n",
    "            Conv1D(4, 15, input_shape = input_shape),\n",
    "            Activation('relu'),\n",
    "            MaxPooling1D(pool_size = 2),\n",
    "\n",
    "            Conv1D(8, 10),\n",
    "            Activation('relu'),\n",
    "            MaxPooling1D(pool_size = 2),\n",
    "\n",
    "            Conv1D(12, 5),\n",
    "            Activation('relu'),\n",
    "            MaxPooling1D(pool_size = 2),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Flatten(),\n",
    "            Dense(256),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.4),\n",
    "\n",
    "            Dense(out_len, activation='softmax')\n",
    "        ], \n",
    "        name = name\n",
    "    )\n",
    "    return CNNmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bidirectional Long-Short Term Memory Neural Network (BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "def make_BiLSTMmodel(input_shape, out_len, name = 'BiLSTM'):\n",
    "    BiLSTMmodel = keras.Sequential(\n",
    "        [\n",
    "            Masking(mask_value = 0., input_shape = input_shape),\n",
    "            \n",
    "            Bidirectional(LSTM(128, return_sequences = True), merge_mode = 'sum'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            AveragePooling1D(4),\n",
    "            Bidirectional(LSTM(128), merge_mode = 'sum'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense((out_len), activation = 'softmax'),\n",
    "        ],\n",
    "        name = name\n",
    "    )\n",
    "    return BiLSTMmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convolutional BiLSTM Neural Network (ConvBiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvBiLSTM\n",
    "def make_ConvBiLSTMmodel(input_shape, out_len, name = 'ConvBiLSTM'):\n",
    "    ConvBiLSTMmodel = keras.Sequential(\n",
    "        [\n",
    "            Masking(mask_value = 0., input_shape = input_shape),\n",
    "                        \n",
    "            Conv1D(128, 3, padding = 'same'),\n",
    "            AveragePooling1D(),\n",
    "\n",
    "            Conv1D(128, 3, padding = 'same'),\n",
    "            AveragePooling1D(),\n",
    "\n",
    "            Conv1D(128, 3, padding = 'same', use_bias = True),\n",
    "            AveragePooling1D(),\n",
    "            Dropout(0.4),\n",
    "            \n",
    "            Bidirectional(LSTM(128, activation = 'tanh'), merge_mode = 'sum'),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            Dense(128, activation = 'relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(out_len, activation = 'softmax')\n",
    "        ], \n",
    "        name = name\n",
    "    )\n",
    "    return ConvBiLSTMmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Attention-based ConvBiLSTM (Read2Pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read2Pheno\n",
    "## Conv & Res net layers\n",
    "CONV_NET_nr, RES_NET_nr, NET_filters, NET_window = 2, 1, 64, 2\n",
    "## extra Dropout layer (after Res block)\n",
    "DROP_r, POOL_s = 0.2, 2\n",
    "## BiLSTM layer\n",
    "LSTM_nodes, MERGE_m = 128, 'sum'\n",
    "## attention Layers\n",
    "ATT_layers, ATT_nodes = 1, 128\n",
    "## fully connected layers\n",
    "FC_layers, FC_nodes, FC_drop = 1, 128, 0.3\n",
    "\n",
    "#####################################################################################################\n",
    "# BLOCK FUNCTIONS\n",
    "def conv_net_block(X, n_cnn_filters = 256, cnn_window = 9, block_name = 'convblock'):\n",
    "    '''\n",
    "    convolutional block with a 1D convolutional layer, a batch norm layer followed by a relu activation.\n",
    "    parameters:\n",
    "        n_cnn_filters: number of output channels\n",
    "        cnn_window: window size of the 1D convolutional layer\n",
    "    '''\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides = 1, padding = 'same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "def res_net_block(X, n_cnn_filters = 256, cnn_window = 9, block_name = 'resblock'):\n",
    "    '''\n",
    "    residual net block accomplished by a few convolutional blocks.\n",
    "    parameters:\n",
    "        n_cnn_filters: number of output channels\n",
    "        cnn_window: window size of the 1D convolutional layer\n",
    "    '''\n",
    "    X_identity = X\n",
    "    # cnn0\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides = 1, padding = 'same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    # cnn1\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides = 1, padding = 'same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    # cnn2\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides = 1, padding = 'same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Add()([X, X_identity])\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "def attention_layer(H_lstm, n_layer, n_node, block_name = 'att'):\n",
    "    '''\n",
    "    feedforward attention layer accomplished by time distributed dense layers.\n",
    "    parameters:\n",
    "        n_layer: number of hidden layers\n",
    "        n_node: number of hidden nodes\n",
    "    '''\n",
    "    H_emb = H_lstm\n",
    "    for i in range(n_layer):\n",
    "        H_lstm = TimeDistributed(Dense(n_node, activation = \"tanh\"))(H_lstm)\n",
    "    M = TimeDistributed(Dense(1, activation = \"linear\"))(H_lstm)\n",
    "    alpha = keras.layers.Softmax(axis = 1)(M)\n",
    "    r_emb = Dot(axes = 1)([alpha, H_emb])\n",
    "    r_emb = Flatten()(r_emb)\n",
    "    return r_emb\n",
    "\n",
    "def fully_connected(r_emb, n_layer, n_node, drop_out_rate = 0.5, block_name = 'fc'):\n",
    "    '''\n",
    "    fully_connected layer consists of a few dense layers.\n",
    "    parameters:\n",
    "        n_layer: number of hidden layers\n",
    "        n_node: number of hidden nodes\n",
    "        drop_out_rate: dropout rate to prevent the model from overfitting\n",
    "    '''\n",
    "    for i in range(n_layer):\n",
    "        r_emb = Dense(n_node, activation = \"relu\")(r_emb)\n",
    "    r_emb = Dropout(drop_out_rate)(r_emb) \n",
    "    return r_emb\n",
    "    \n",
    "#####################################################################################################\n",
    "# MODEL COMPILING FUNCTION\n",
    "\n",
    "def make_R2Pmodel(input_shape, out_len, name = 'Read2Pheno'):\n",
    "    X = Input(shape = input_shape)\n",
    "    X_mask = Masking(mask_value = 0.)(X)\n",
    "\n",
    "    ## CONV Layers\n",
    "    X_cnn = X_mask\n",
    "    # conv_net\n",
    "    for i in range(CONV_NET_nr):\n",
    "        X_cnn = conv_net_block(\n",
    "            X_cnn, \n",
    "            n_cnn_filters = NET_filters, \n",
    "            cnn_window = NET_window\n",
    "            )\n",
    "    # res_net\n",
    "    for i in range(RES_NET_nr):\n",
    "        X_cnn = res_net_block(\n",
    "            X_cnn, \n",
    "            n_cnn_filters = NET_filters, \n",
    "            cnn_window = NET_window\n",
    "            )\n",
    "\n",
    "    ## Extra Pooling layer and Dropout\n",
    "    X_pool = AveragePooling1D(pool_size = POOL_s)(X_cnn)\n",
    "    X_drop = Dropout(DROP_r)(X_pool)\n",
    "\n",
    "    ## RNN Layers\n",
    "    H_lstm = Bidirectional(LSTM(LSTM_nodes, return_sequences = True), merge_mode = MERGE_m)(X_drop)\n",
    "    H_lstm = Activation('tanh')(H_lstm)\n",
    "\n",
    "    ## ATT Layers\n",
    "    r_emb = attention_layer(\n",
    "        H_lstm, \n",
    "        n_layer = ATT_layers, \n",
    "        n_node = ATT_nodes, \n",
    "        block_name = 'att'\n",
    "        )    \n",
    "    # Fully connected layers\n",
    "    r_emb = fully_connected(\n",
    "        r_emb, \n",
    "        n_layer = FC_layers, \n",
    "        n_node = FC_nodes, \n",
    "        drop_out_rate = FC_drop, \n",
    "        block_name = 'fc'\n",
    "        )\n",
    "\n",
    "    # Compile model\n",
    "    out = Dense(out_len, activation = 'softmax', name = 'final_dense')(r_emb)\n",
    "    R2Pmodel = Model(inputs = X, outputs = out, name = name)\n",
    "    \n",
    "    return R2Pmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Deep Learning models\n",
    "The models are generated based on input and output shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CNN\n",
    "The CNN models are created, tailored to the different input (k-mer) and output (taxon) shapes. The input shape is determined by the k-mer used (with associated matrix) and the output shape (and thus the number of nodes in the final dense layer) is determined by the amount of unique taxon labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# for Family\n",
    "CNN_fam_3 = make_CNNmodel(\n",
    "    input_shape = INPUT_SHAPE_3_MER, \n",
    "    out_len = fam_count, \n",
    "    name = 'CNN_fam_3'\n",
    "    )\n",
    "CNN_fam_5 = make_CNNmodel(\n",
    "    input_shape = INPUT_SHAPE_5_MER, \n",
    "    out_len = fam_count, \n",
    "    name = 'CNN_fam_5'\n",
    "    )\n",
    "CNN_fam_7 = make_CNNmodel(\n",
    "    input_shape = INPUT_SHAPE_7_MER, \n",
    "    out_len = fam_count, \n",
    "    name = 'CNN_fam_7'\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# for Genus\n",
    "CNN_gen_3 = make_CNNmodel(\n",
    "    input_shape = INPUT_SHAPE_3_MER, \n",
    "    out_len = gen_count, \n",
    "    name = 'CNN_gen_3'\n",
    "    )\n",
    "CNN_gen_5 = make_CNNmodel(\n",
    "    input_shape = INPUT_SHAPE_5_MER, \n",
    "    out_len = gen_count, \n",
    "    name = 'CNN_gen_5'\n",
    "    )\n",
    "CNN_gen_7 = make_CNNmodel(\n",
    "    input_shape = INPUT_SHAPE_7_MER, \n",
    "    out_len = gen_count, \n",
    "    name = 'CNN_gen_7'\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# for Species\n",
    "CNN_spe_3 = make_CNNmodel(\n",
    "    input_shape = INPUT_SHAPE_3_MER, \n",
    "    out_len = spe_count, \n",
    "    name = 'CNN_spe_3'\n",
    "    )\n",
    "CNN_spe_5 = make_CNNmodel(\n",
    "    input_shape = INPUT_SHAPE_5_MER, \n",
    "    out_len = spe_count, \n",
    "    name = 'CNN_spe_5'\n",
    "    )\n",
    "CNN_spe_7 = make_CNNmodel(\n",
    "    input_shape = INPUT_SHAPE_7_MER, \n",
    "    out_len = spe_count, \n",
    "    name = 'CNN_spe_7'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RNN\n",
    "The RNN models are created, tailored to the different input (sequence length) and output (taxon) shapes. The input shape is determined by the length of the sequences used (V-region selected or not) and the output shape (and thus the number of nodes in the final dense layer) is determined by the amount of unique taxon labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# for Family (output length = number of unique labels)\n",
    "BiLSTM_fam = make_BiLSTMmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN,\n",
    "    out_len = fam_count, \n",
    "    name = 'BiLSTM_fam'\n",
    "    )\n",
    "ConvBiLSTM_fam = make_ConvBiLSTMmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN,\n",
    "    out_len = fam_count\n",
    "    , name = 'ConvBiLSTM_fam'\n",
    "    )\n",
    "R2P_fam = make_R2Pmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN,\n",
    "    out_len = fam_count, \n",
    "    name = 'R2P_fam'\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# for Genus\n",
    "BiLSTM_gen = make_BiLSTMmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN,\n",
    "    out_len = gen_count, \n",
    "    name = 'BiLSTM_gen'\n",
    "    )\n",
    "ConvBiLSTM_gen = make_ConvBiLSTMmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN,\n",
    "    out_len = gen_count, \n",
    "    name = 'ConvBiLSTM_gen'\n",
    "    )\n",
    "R2P_gen = make_R2Pmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN,\n",
    "    out_len = gen_count, \n",
    "    name='R2P_gen'\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# for Species\n",
    "BiLSTM_spe = make_BiLSTMmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN,\n",
    "    out_len = spe_count, \n",
    "    name = 'BiLSTM_spe'\n",
    "    )\n",
    "ConvBiLSTM_spe = make_ConvBiLSTMmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN,\n",
    "    out_len = spe_count, \n",
    "    name = 'ConvBiLSTM_spe'\n",
    "    )\n",
    "R2P_spe = make_R2Pmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN,\n",
    "    out_len = spe_count, \n",
    "    name='R2P_spe'\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "# for Genus with V-region selected data\n",
    "BiLSTM_gen_V = make_BiLSTMmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN_V,\n",
    "    out_len = gen_count, \n",
    "    name = 'BiLSTM_gen_V'\n",
    "    )\n",
    "ConvBiLSTM_gen_V = make_ConvBiLSTMmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN_V,\n",
    "    out_len = gen_count, \n",
    "    name = 'ConvBiLSTM_gen_V'\n",
    "    )\n",
    "R2P_gen_V = make_R2Pmodel(\n",
    "    input_shape = INPUT_SHAPE_RNN_V,\n",
    "    out_len = gen_count, \n",
    "    name='R2P_gen_V'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling, Training and Evaluating the Deep Learning models\n",
    "\n",
    "For each model:\n",
    "- a Weights and Biases run is initiated\n",
    "- the model is compiled and a summary is printed\n",
    "- the model is trained and training-time is measured\n",
    "- the model is evaluated by calculating the test-loss and -accuracy, the F1 score and the MCC score\n",
    "- the training history and metrics are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model, \n",
    "    train_data, train_labels, \n",
    "    validation_data, validation_labels, \n",
    "    test_data, test_labels\n",
    "    ):\n",
    "\n",
    "    wandb.init(project = 'Final Training', entity = 'bachelorprojectgroup9', name = model.name)\n",
    "\n",
    "    # LOADING MODEL\n",
    "    print ('Loading {} model...'.format(model.name))\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=Adam(learning_rate=LR), \n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    print(model.summary())\n",
    "\n",
    "    # FITTING MODEL\n",
    "    print ('Fitting {} model...'.format(model.name))\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_data, train_labels, \n",
    "        epochs = EPOCHS, batch_size=BATCH_SIZE, \n",
    "        validation_data = (validation_data, validation_labels), \n",
    "        callbacks = [WandbCallback()]\n",
    "        )\n",
    "    time_taken = round(time.time() - start_time)\n",
    "    # history object is saved and can later be destinguished using the model/train_data names\n",
    "    np.save('history/{}_{}.npy'.format(model.name, argname('train_data')), history.history)\n",
    "    \n",
    "    # EVALUATING MODEL\n",
    "    print ('Evaluating {} model...'.format(model.name))\n",
    "    test_labels_arg = np.argmax(test_labels, axis = 1)\n",
    "    test_predictions = np.argmax(model.predict(test_data), axis = 1)\n",
    "    loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "    # F1-score: harmonic mean of the precision and recall\n",
    "    #   score from 0 to 1\n",
    "    f1 = f1_score(y_true = test_labels_arg, y_pred = test_predictions, average = 'weighted')\n",
    "    # Matthews correlation coefficient: coefficient of +1 represents a perfect prediction,\n",
    "    #   0 an average random prediction and -1 an inverse prediction\n",
    "    mcc = matthews_corrcoef(y_true = test_labels_arg, y_pred = test_predictions)\n",
    "\n",
    "    score_dict = pd.DataFrame({\n",
    "        'Model/run' : model.name, \n",
    "        'Data' : argname('train_data'), \n",
    "        'Training time' : time_taken, \n",
    "        'Test loss' : loss, \n",
    "        'Test accuracy' : accuracy, \n",
    "        'F1-score' : f1, \n",
    "        'MCC' : mcc}, \n",
    "        index=[0]\n",
    "        )\n",
    "    print(score_dict)\n",
    "    # score metrics are saved and can later be destinguished using the model names\n",
    "    score_dict.to_csv('scores/{}_evaluation.csv'.format(model.name), index = False)\n",
    "\n",
    "    wandb.finish()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the CNN models and saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING CNN MODELS\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "# running the CNN model at genus level \n",
    "# with 3-, 5- and 7-mer encoding\n",
    "# on the non-augmented data\n",
    "train_and_evaluate_model(\n",
    "    CNN_gen_3, \n",
    "    x_train_CNN_na3, y_train_gen_na, \n",
    "    dataval_CNN_na3, labelsval_gen_na, \n",
    "    x_test_CNN_na3, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    CNN_gen_5, \n",
    "    x_train_CNN_na5, y_train_gen_na, \n",
    "    dataval_CNN_na5, labelsval_gen_na, \n",
    "    x_test_CNN_na5, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    CNN_gen_7, \n",
    "    x_train_CNN_na7, y_train_gen_na, \n",
    "    dataval_CNN_na7, labelsval_gen_na, \n",
    "    x_test_CNN_na7, y_test_gen_na\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "# running the CNN model at family and species level\n",
    "# with 7-mer encoding\n",
    "# on the non-augmented data\n",
    "train_and_evaluate_model(\n",
    "    CNN_fam_7, \n",
    "    x_train_CNN_na7, y_train_fam_na, \n",
    "    dataval_CNN_na7, labelsval_fam_na, \n",
    "    x_test_CNN_na7, y_test_fam_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    CNN_spe_7, \n",
    "    x_train_CNN_na7, y_train_spe_na, \n",
    "    dataval_CNN_na7, labelsval_spe_na, \n",
    "    x_test_CNN_na7, y_test_spe_na\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "# running the CNN model at genus level \n",
    "# with 7-mer encoding\n",
    "# on the augmented data\n",
    "train_and_evaluate_model(\n",
    "    CNN_gen_7, \n",
    "    x_train_CNN_a7, y_train_gen_a, \n",
    "    dataval_CNN_a7, labelsval_gen_a, \n",
    "    x_test_CNN_na7, y_test_gen_na\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "# running the CNN model at genus level \n",
    "# with 7-mer encoding\n",
    "# on the V-region selected data\n",
    "train_and_evaluate_model(\n",
    "    CNN_gen_7, \n",
    "    x_train_CNN_v7, y_train_gen_na, \n",
    "    dataval_CNN_v7, labelsval_gen_na, \n",
    "    x_test_CNN_v7, y_test_gen_na\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the RNN models and saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING RNN MODELS\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "# running the RNN models at genus level \n",
    "# with regualar and mutation rate adjusted one-hot-encoding\n",
    "# on the non-augmented data\n",
    "train_and_evaluate_model(\n",
    "    BiLSTM_gen, \n",
    "    x_train_RNN_na0, y_train_gen_na, \n",
    "    dataval_RNN_na0, labelsval_gen_na, \n",
    "    x_test_RNN_na0, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    ConvBiLSTM_gen, \n",
    "    x_train_RNN_na0, y_train_gen_na, \n",
    "    dataval_RNN_na0, labelsval_gen_na, \n",
    "    x_test_RNN_na0, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    R2P_gen, \n",
    "    x_train_RNN_na0, y_train_gen_na, \n",
    "    dataval_RNN_na0, labelsval_gen_na, \n",
    "    x_test_RNN_na0, y_test_gen_na\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "train_and_evaluate_model(\n",
    "    BiLSTM_gen, \n",
    "    x_train_RNN_na1, y_train_gen_na, \n",
    "    dataval_RNN_na1, labelsval_gen_na, \n",
    "    x_test_RNN_na1, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    ConvBiLSTM_gen, \n",
    "    x_train_RNN_na1, y_train_gen_na, \n",
    "    dataval_RNN_na1, labelsval_gen_na, \n",
    "    x_test_RNN_na1, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    R2P_gen, \n",
    "    x_train_RNN_na1, y_train_gen_na, \n",
    "    dataval_RNN_na1, labelsval_gen_na, \n",
    "    x_test_RNN_na1, y_test_gen_na\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "# running the RNN models at family and species level \n",
    "# with the mutation rate adjusted one-hot-encoding\n",
    "# on the non-augmented data\n",
    "train_and_evaluate_model(\n",
    "    BiLSTM_fam, \n",
    "    x_train_RNN_na1, y_train_fam_na, \n",
    "    dataval_RNN_na1, labelsval_fam_na, \n",
    "    x_test_RNN_na1, y_test_fam_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    ConvBiLSTM_fam, \n",
    "    x_train_RNN_na1, y_train_fam_na, \n",
    "    dataval_RNN_na1, labelsval_fam_na, \n",
    "    x_test_RNN_na1, y_test_fam_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    R2P_fam, \n",
    "    x_train_RNN_na1, y_train_fam_na, \n",
    "    dataval_RNN_na1, labelsval_fam_na, \n",
    "    x_test_RNN_na1, y_test_fam_na\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "train_and_evaluate_model(\n",
    "    BiLSTM_spe, \n",
    "    x_train_RNN_na1, y_train_spe_na, \n",
    "    dataval_RNN_na1, labelsval_spe_na, \n",
    "    x_test_RNN_na1, y_test_spe_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    ConvBiLSTM_spe, \n",
    "    x_train_RNN_na1, y_train_spe_na, \n",
    "    dataval_RNN_na1, labelsval_spe_na, \n",
    "    x_test_RNN_na1, y_test_spe_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    R2P_spe, \n",
    "    x_train_RNN_na1, y_train_spe_na, \n",
    "    dataval_RNN_na1, labelsval_spe_na, \n",
    "    x_test_RNN_na1, y_test_spe_na\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "# running the RNN models at genus level \n",
    "# with the mutation rate adjusted one-hot-encoding \n",
    "# on the augmented data\n",
    "train_and_evaluate_model(\n",
    "    BiLSTM_gen, \n",
    "    x_train_RNN_a1, y_train_gen_a, \n",
    "    dataval_RNN_a1, labelsval_gen_a, \n",
    "    x_test_RNN_na1, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    ConvBiLSTM_gen, \n",
    "    x_train_RNN_a1, y_train_gen_a, \n",
    "    dataval_RNN_a1, labelsval_gen_a, \n",
    "    x_test_RNN_na1, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    R2P_gen, \n",
    "    x_train_RNN_a1, y_train_gen_a, \n",
    "    dataval_RNN_a1, labelsval_gen_a, \n",
    "    x_test_RNN_na1, y_test_gen_na\n",
    "    )\n",
    "# ----------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "# running the RNN models at genus level \n",
    "# with the mutation rate adjusted one-hot-encoding \n",
    "# on the V-region selected data\n",
    "train_and_evaluate_model(\n",
    "    BiLSTM_gen_V, \n",
    "    x_train_RNN_v1, y_train_gen_na, \n",
    "    dataval_RNN_v1, labelsval_gen_na, \n",
    "    x_test_RNN_v1, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    ConvBiLSTM_gen_V, \n",
    "    x_train_RNN_v1, y_train_gen_na, \n",
    "    dataval_RNN_v1, labelsval_gen_na, \n",
    "    x_test_RNN_v1, y_test_gen_na\n",
    "    )\n",
    "train_and_evaluate_model(\n",
    "    R2P_gen_V, \n",
    "    x_train_RNN_v1, y_train_gen_na, \n",
    "    dataval_RNN_v1, labelsval_gen_na, \n",
    "    x_test_RNN_v1, y_test_gen_na\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ribosomal Database Project (RDP)\n",
    "Train and validation dataset are merged into one, this is becuase the RDP machine learning model does not require validation steps during training. The databases are converted to ready4train taxonomy and fasta files to be fed into the RDP Classifier.\n",
    "\n",
    "When running cells, new files will be saved in \"RDPfiles\" directory at the same folder of this jupyter notebook is located. Directory name can be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting variables and loading data\n",
    "Training and classification is done with the whole taxon hierarchy, from kingdom to species, but a specific level can be chosen during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global RDPfiles\n",
    "import pandas as pd\n",
    "# runs to do\n",
    "#   - genus level with 3 data types (non-augmented, augmented, V-selected)\n",
    "#   - family and species level with non-augmented\n",
    "\n",
    "# variables\n",
    "RDPfiles = \"RDPfiles\"\n",
    "classifier_loc = \"rdptools/classifier.jar\"\n",
    "confidence_score = 0.8\n",
    "level = 'genus'             # used for evaluation\n",
    "model_run = 'RDP_gen'       # used for naming the run (change taxon)\n",
    "data = 'x_train_RDP_naX'    # used for naming the data (change na/n/v)\n",
    "seq_col = -1                # used for selecting sequence (change -1/-2 for V_seq col)\n",
    "is_v = -1                   # used for selecting taxa (change to -2 if there is a V_seq col)\n",
    "# loading data\n",
    "train_data = pd.read.csv('df_train_0.csv')  # change 1/0 for a/na data\n",
    "val_data = pd.read_csv('df_val_0.csv')      # change 1/0 for a/na data\n",
    "test_data = pd.read_csv('df_test_0.csv')\n",
    "\n",
    "print('Variables set and datasets loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing functions\n",
    "**Lineage2taxTrain** converts tab separated taxonomy text files into the ready4train_taxonomy.txt file. This text file contains the hierarchical taxonomy information in the following format: tax ID | taxon name | parent taxid | depth | rank.\n",
    "- Tax ID is the index of the rank in the taxonomy file\n",
    "- Taxon name is the name for the taxonomic rank\n",
    "- Parent taxid is the tax ID of the rank above the current rank\n",
    "- Depth is the depth of the rank. Depth 0 is always root. The kingdom rank has depth of 1\n",
    "- Rank is the taxonomic ranks\n",
    "\n",
    "**AddFullLineage** generates the ready4train_seqs.fasta file. It has structure similar to the sequence fasta file, but semicolon separated taxonomy is added next to sequence ID.\n",
    "\n",
    "\n",
    "**RDPoutput2score** generates the accuracy, F1 score and MCC using output.txt and test_taxonomy.txt. Output.txt is generated when classifying test_sequences.txt with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineage2taxTrain(raw_taxons):\n",
    "    taxons_list = raw_taxons.strip().split('\\n')\n",
    "    header = taxons_list[0].split('\\t')[1:] # headers = list of ranks\n",
    "    hash = {} # taxon name-id map\n",
    "    ranks = {} # column number-rank map\n",
    "    lineages = [] # list of unique lineages\n",
    "\n",
    "    with open(\"{}/ready4train_taxonomy.txt\".format(RDPfiles), \"w\") as f:\n",
    "        # initiate root rank taxon id map\n",
    "        hash = {\"Root\":0}\n",
    "        for i in range(len(header)):\n",
    "            name = header[i]\n",
    "            ranks[i] = name\n",
    "\n",
    "        # root rank info\n",
    "        root = ['0', 'Root', '-1', '0', 'rootrank']\n",
    "        f.write(\"*\".join(root) +  '\\n')\n",
    "\n",
    "        ID = 0\n",
    "        for line in taxons_list[1:]:\n",
    "            cols = line.strip().split('\\t')[1:]\n",
    "            # iterate each column\n",
    "            for i in range(len(cols)):\n",
    "                name = []\n",
    "                for node in cols[:i + 1]:\n",
    "                    node = node.strip()\n",
    "                    if not node in ('-', ''):\n",
    "                        name.append(node)\n",
    "\n",
    "                pName = \";\".join(name[:-1])\n",
    "                if not name in lineages:\n",
    "                    lineages.append(name)\n",
    "\n",
    "                depth = len(name)\n",
    "                name = ';'.join(name)\n",
    "                if name in hash.keys():\n",
    "                    # already seen this lineage\n",
    "                    continue\n",
    "                try:\n",
    "                    rank = ranks[i]\n",
    "                except KeyError:\n",
    "                    print (cols)\n",
    "                    sys.exit()\n",
    "\n",
    "                if i == 0:\n",
    "                    pName = 'Root'\n",
    "                # parent taxid\n",
    "                pID = hash[pName]\n",
    "                ID += 1\n",
    "                # add name-id to the map\n",
    "                hash[name] = ID\n",
    "                out = ['%s'%ID, name.split(';')[-1], '%s'%pID, '%s'%depth, rank]\n",
    "                f.write(\"*\".join(out) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def addFullLineage(raw_taxons, raw_seqs):\n",
    "    # lineage map\n",
    "    hash = {}\n",
    "    taxonomy_list = raw_taxons.strip().split('\\n')\n",
    "\n",
    "    for line in taxonomy_list[1:]:\n",
    "        line = line.strip()\n",
    "        cols = line.strip().split('\\t')\n",
    "        lineage = ['Root']\n",
    "\n",
    "        for node in cols[1:]:\n",
    "            node = node.strip()\n",
    "            if not (node == '-' or node == ''):\n",
    "                lineage.append(node)\n",
    "\n",
    "        ID = cols[0]\n",
    "        lineage = ';'.join(lineage).strip()\n",
    "        hash[ID] = lineage\n",
    "\n",
    "    sequence_list = raw_seqs.strip().split('\\n')\n",
    "    with open(\"{}/ready4train_seqs.fasta\".format(RDPfiles), \"w\") as f:\n",
    "        for line in sequence_list:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                continue\n",
    "            if line[0] == '>':\n",
    "                ID = line.strip().split()[0].replace('>', '')\n",
    "                lineage = hash[ID]\n",
    "                f.write('>' + ID + '\\t' + lineage + '\\n')\n",
    "            else:\n",
    "                f.write(line.strip() + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def RDPoutput2score(pred_file, true_file, level, cf):\n",
    "    taxon_list = []\n",
    "    ranks = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "    level = ranks.index(level)\n",
    "\n",
    "    pred = pd.read_csv(pred_file, sep=\"\\t\", header=None)\n",
    "    pred.drop(pred.columns[1:level+5+2*level],  axis = 'columns', inplace=True)\n",
    "    pred.drop(pred.columns[4:], axis = 'columns', inplace=True)\n",
    "    \n",
    "    pred_dict = {}\n",
    "    for index, row in pred.iterrows():\n",
    "        row = row.tolist()\n",
    "        if row[1] not in taxon_list:\n",
    "            taxon_list += [row[1]]\n",
    "        if float(row[3]) >= cf:\n",
    "            pred_dict[row[0]] = row[1]\n",
    "\n",
    "    true = pd.read_csv(true_file, sep=\"\\t\", header=None)\n",
    "    true_dict = {}\n",
    "    for index, row in true.iterrows():\n",
    "        true_dict[row[0]] = row[level+1]\n",
    "        if row[level+1] not in taxon_list:\n",
    "            taxon_list += [row[level+1]]\n",
    "\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for i in pred_dict.keys():\n",
    "        y_pred.append(taxon_list.index(pred_dict[i]))\n",
    "        y_true.append(taxon_list.index(true_dict[i]))\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    score_dict = pd.DataFrame({\n",
    "        'Model/run' : model_run,    # 'RDP_gen'\n",
    "        'Data' : data,              # 'x_train_RDP_naX'\n",
    "        'Training time' : None, \n",
    "        'Test loss' : None, \n",
    "        'Test accuracy' : acc, \n",
    "        'F1-score' : f1, \n",
    "        'MCC' : mcc}, \n",
    "        index=[0]\n",
    "        )\n",
    "    print(score_dict)\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RDP Main\n",
    "In this cell, a new directory is made to save output.\n",
    "\n",
    "- Train and validation sets are merged into one training set.\n",
    "- raw_seqs + raw_taxons -> ready4train files (input for training the RDP model)\n",
    "- test dataset -> test_sequcnes.fasta + test_taxonomy.txt (used in evaluation of new trained models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing for RDP completed\n"
     ]
    }
   ],
   "source": [
    "# main_RDP.py\n",
    "os.system(\"mkdir {}\".format(RDPfiles))\n",
    "\n",
    "# merge train and validation dataframes into one.\n",
    "train = pd.concat([train_data, val_data], ignore_index = True)\n",
    "test = test_data\n",
    "\n",
    "# convert train and test dataframe into tab separated taxonomy and sequence string\n",
    "# taxnomy file is converted to a tab separated string\n",
    "# sequence file is converted to fasta format with sequence ID and sequence\n",
    "raw_seqs = ''\n",
    "raw_taxons = 'SeqId Kingdom\tPhylum\tClass\tOrder\tFamily\tGenus\tSpecies' + '\\n'\n",
    "for index, row in train.iterrows():\n",
    "    taxons = row.tolist()\n",
    "    raw_seqs += '>' + taxons[0] + '\\n' + taxons[seq_col] + '\\n'\n",
    "    raw_taxons += '\\t'.join(taxons[:is_v]) + '\\n'\n",
    "\n",
    "# convert test dataframe into text and fasta files to be utilized by RDP\n",
    "# taxnomy file is converted to a tab separated text file\n",
    "# sequence file is converted to fasta format\n",
    "with open(\"{}/test_sequences.fasta\".format(RDPfiles), \"w\") as seq_f, open(\n",
    "    \"{}/test_taxonomy.txt\".format(RDPfiles), \"w\") as tax_f:\n",
    "\n",
    "    for index, row in test.iterrows():\n",
    "        taxons = row.tolist()\n",
    "        seq_f.write('>' + taxons[0] + '\\n' + taxons[seq_col] + '\\n')\n",
    "        tax_f.write('\\t'.join(taxons[:is_v]) + '\\n')\n",
    "        \n",
    "    seq_f.close()\n",
    "    tax_f.close()\n",
    "\n",
    "# convert raw taxonomy and sequence files to ready4rdp trainable files\n",
    "lineage2taxTrain(raw_taxons)\n",
    "addFullLineage(raw_taxons, raw_seqs)\n",
    "\n",
    "print(\"Data preprocessing for RDP completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is performed with the train() function. The model is saved in the training_files directory with four weight files. New file, rRNAClassifier.properties, are necessary for bridging these files and the RDP Classifier. Classification is performed with the classify() function. Option -o leads RDP Classifier to use the newly generated training models.\n",
    "\n",
    "Using RDPoutput2score, the accuracy, F1 score and MCC are calculated. These values will be used to compare with the deep learning models. User can choose specific level of taxon to evaluate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDP training-time: 85 seconds\n",
      "accuracy: 0.9724941724941725\n",
      "F1-score: 0.9726149488062451\n",
      "MCC-score: 0.9723559927778965\n"
     ]
    }
   ],
   "source": [
    "# Training the RDP classifier\n",
    "start_time = time.time()\n",
    "os.system(\"java -Xmx10g -jar {} train -o {}/training_files -s {}/ready4train_seqs.fasta -t {}/ready4train_taxonomy.txt\".format(\n",
    "    classifier_loc, RDPfiles, RDPfiles, RDPfiles))\n",
    "\n",
    "with open(\"{}/training_files/rRNAClassifier.properties\".format(RDPfiles), \"w\") as f:\n",
    "    f.write(\"bergeyTree=bergeyTrainingTree.xml\\nprobabilityList=genus_wordConditionalProbList.txt\\nprobabilityIndex=wordConditionalProbIndexArr.txt\\nwordPrior=logWordPrior.txt\\nclassifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.5, May 2012 \")\n",
    "    f.close()\n",
    "\n",
    "time_taken = round(time.time() - start_time)\n",
    "print(\"RDP training-time: {} seconds\".format(time_taken))\n",
    "\n",
    "# Testing the RDP classifier\n",
    "os.system(\"java -Xmx10g -jar {} classify -t {}/training_files/rRNAClassifier.properties  -o {}/output.txt {}/test_sequences.fasta\".format(\n",
    "    classifier_loc, RDPfiles, RDPfiles, RDPfiles))\n",
    "\n",
    "# Evaluating the RDP classifier and save the results\n",
    "score_dict = RDPoutput2score(\"{}/output.txt\".format(RDPfiles), \n",
    "\"{}/test_taxonomy.txt\".format(RDPfiles), level, confidence_score)\n",
    "\n",
    "score_dict.at[0, 'Training time'] = time_taken\n",
    "score_dict.to_csv('scores/RDP_evaluation', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining and Visualizing the results\n",
    "### Loading the saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required modules\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the csv_files containing the scores are loaded and the resulting dataframe is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model/run</th>\n",
       "      <th>Data</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Test loss</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BiLSTM_gen</td>\n",
       "      <td>x_train_RNN_na0</td>\n",
       "      <td>2677</td>\n",
       "      <td>2.569284</td>\n",
       "      <td>0.434009</td>\n",
       "      <td>0.364383</td>\n",
       "      <td>0.429371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN_gen_6</td>\n",
       "      <td>x_train_CNN_na6</td>\n",
       "      <td>346</td>\n",
       "      <td>0.270210</td>\n",
       "      <td>0.924773</td>\n",
       "      <td>0.913211</td>\n",
       "      <td>0.924266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN_gen_3</td>\n",
       "      <td>x_train_CNN_na3</td>\n",
       "      <td>193</td>\n",
       "      <td>0.923764</td>\n",
       "      <td>0.759431</td>\n",
       "      <td>0.732084</td>\n",
       "      <td>0.757753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN_fam_6</td>\n",
       "      <td>x_train_CNN_na6</td>\n",
       "      <td>224</td>\n",
       "      <td>0.088498</td>\n",
       "      <td>0.980857</td>\n",
       "      <td>0.979135</td>\n",
       "      <td>0.980552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ConvBiLSTM_gen</td>\n",
       "      <td>x_train_RNN_na0</td>\n",
       "      <td>628</td>\n",
       "      <td>1.829659</td>\n",
       "      <td>0.580320</td>\n",
       "      <td>0.531201</td>\n",
       "      <td>0.577239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN_spe_6</td>\n",
       "      <td>x_train_CNN_na6</td>\n",
       "      <td>225</td>\n",
       "      <td>0.607089</td>\n",
       "      <td>0.831412</td>\n",
       "      <td>0.799931</td>\n",
       "      <td>0.830663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2P_gen</td>\n",
       "      <td>x_train_RNN_na0</td>\n",
       "      <td>1443</td>\n",
       "      <td>1.420991</td>\n",
       "      <td>0.662935</td>\n",
       "      <td>0.610153</td>\n",
       "      <td>0.660651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model/run             Data  Training time  Test loss  Test accuracy  \\\n",
       "0      BiLSTM_gen  x_train_RNN_na0           2677   2.569284       0.434009   \n",
       "0       CNN_gen_6  x_train_CNN_na6            346   0.270210       0.924773   \n",
       "0       CNN_gen_3  x_train_CNN_na3            193   0.923764       0.759431   \n",
       "0       CNN_fam_6  x_train_CNN_na6            224   0.088498       0.980857   \n",
       "0  ConvBiLSTM_gen  x_train_RNN_na0            628   1.829659       0.580320   \n",
       "0       CNN_spe_6  x_train_CNN_na6            225   0.607089       0.831412   \n",
       "0         R2P_gen  x_train_RNN_na0           1443   1.420991       0.662935   \n",
       "\n",
       "   F1-score       MCC  \n",
       "0  0.364383  0.429371  \n",
       "0  0.913211  0.924266  \n",
       "0  0.732084  0.757753  \n",
       "0  0.979135  0.980552  \n",
       "0  0.531201  0.577239  \n",
       "0  0.799931  0.830663  \n",
       "0  0.610153  0.660651  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # specifying the path to csv files\n",
    "path = \"scores\"\n",
    "# csv files in the path\n",
    "files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "# defining an empty DataFrame\n",
    "col_names = ['Model/run', 'Data', 'Training time', 'Test loss', 'Test accuracy', 'F1-score', 'MCC']\n",
    "score_df = pd.DataFrame(columns = col_names)\n",
    "# defining an empty list to store content\n",
    "content = []\n",
    "\n",
    "for filename in files:\n",
    "    # reading content of csv file\n",
    "    df = pd.read_csv(filename, index_col = None)\n",
    "    content.append(df)\n",
    "  \n",
    "# converting content to one data frame\n",
    "score_df = pd.concat(content)\n",
    "score_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Taxon</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Data</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Test loss</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>gen</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "      <td>2677</td>\n",
       "      <td>2.569284</td>\n",
       "      <td>0.434009</td>\n",
       "      <td>0.364383</td>\n",
       "      <td>0.429371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>gen</td>\n",
       "      <td>6</td>\n",
       "      <td>na</td>\n",
       "      <td>346</td>\n",
       "      <td>0.270210</td>\n",
       "      <td>0.924773</td>\n",
       "      <td>0.913211</td>\n",
       "      <td>0.924266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>gen</td>\n",
       "      <td>3</td>\n",
       "      <td>na</td>\n",
       "      <td>193</td>\n",
       "      <td>0.923764</td>\n",
       "      <td>0.759431</td>\n",
       "      <td>0.732084</td>\n",
       "      <td>0.757753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>fam</td>\n",
       "      <td>6</td>\n",
       "      <td>na</td>\n",
       "      <td>224</td>\n",
       "      <td>0.088498</td>\n",
       "      <td>0.980857</td>\n",
       "      <td>0.979135</td>\n",
       "      <td>0.980552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ConvBiLSTM</td>\n",
       "      <td>gen</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "      <td>628</td>\n",
       "      <td>1.829659</td>\n",
       "      <td>0.580320</td>\n",
       "      <td>0.531201</td>\n",
       "      <td>0.577239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>spe</td>\n",
       "      <td>6</td>\n",
       "      <td>na</td>\n",
       "      <td>225</td>\n",
       "      <td>0.607089</td>\n",
       "      <td>0.831412</td>\n",
       "      <td>0.799931</td>\n",
       "      <td>0.830663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2P</td>\n",
       "      <td>gen</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "      <td>1443</td>\n",
       "      <td>1.420991</td>\n",
       "      <td>0.662935</td>\n",
       "      <td>0.610153</td>\n",
       "      <td>0.660651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model Taxon Encoding Data  Training time  Test loss  Test accuracy  \\\n",
       "0      BiLSTM   gen        0   na           2677   2.569284       0.434009   \n",
       "0         CNN   gen        6   na            346   0.270210       0.924773   \n",
       "0         CNN   gen        3   na            193   0.923764       0.759431   \n",
       "0         CNN   fam        6   na            224   0.088498       0.980857   \n",
       "0  ConvBiLSTM   gen        0   na            628   1.829659       0.580320   \n",
       "0         CNN   spe        6   na            225   0.607089       0.831412   \n",
       "0         R2P   gen        0   na           1443   1.420991       0.662935   \n",
       "\n",
       "   F1-score       MCC  \n",
       "0  0.364383  0.429371  \n",
       "0  0.913211  0.924266  \n",
       "0  0.732084  0.757753  \n",
       "0  0.979135  0.980552  \n",
       "0  0.531201  0.577239  \n",
       "0  0.799931  0.830663  \n",
       "0  0.610153  0.660651  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = score_df.copy()\n",
    "\n",
    "# splitting the column values in seperate, more informative columns\n",
    "df.insert(loc = 0, column = 'Encoding', \n",
    "value = df['Data'].apply(lambda x: x.split('_')[-1][-1]))\n",
    "df.insert(loc = 0, column = 'Taxon', \n",
    "value = df['Model/run'].apply(lambda x: x.split('_')[1]))\n",
    "df.insert(loc = 0, column = 'Model', \n",
    "value = df['Model/run'].apply(lambda x: x.split('_')[0]))\n",
    "\n",
    "df['Data'] = score_df['Data'].apply(lambda x: x.split('_')[-1][:2])\n",
    "\n",
    "# removing the columns that are no longer necessary\n",
    "df.drop('Model/run', axis=1, inplace=True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Taxon</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Data</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Test loss</th>\n",
       "      <th>Test accuracy (%)</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Genus</td>\n",
       "      <td>3-mer</td>\n",
       "      <td>non-augmented</td>\n",
       "      <td>0:3:13</td>\n",
       "      <td>0.9238</td>\n",
       "      <td>75.94</td>\n",
       "      <td>0.7321</td>\n",
       "      <td>0.7578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Genus</td>\n",
       "      <td>6-mer</td>\n",
       "      <td>non-augmented</td>\n",
       "      <td>0:5:46</td>\n",
       "      <td>0.2702</td>\n",
       "      <td>92.48</td>\n",
       "      <td>0.9132</td>\n",
       "      <td>0.9243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Family</td>\n",
       "      <td>6-mer</td>\n",
       "      <td>non-augmented</td>\n",
       "      <td>0:3:44</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>98.09</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Species</td>\n",
       "      <td>6-mer</td>\n",
       "      <td>non-augmented</td>\n",
       "      <td>0:3:45</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>83.14</td>\n",
       "      <td>0.7999</td>\n",
       "      <td>0.8307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>Genus</td>\n",
       "      <td>regular one-hot</td>\n",
       "      <td>non-augmented</td>\n",
       "      <td>0:44:37</td>\n",
       "      <td>2.5693</td>\n",
       "      <td>43.40</td>\n",
       "      <td>0.3644</td>\n",
       "      <td>0.4294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ConvBiLSTM</td>\n",
       "      <td>Genus</td>\n",
       "      <td>regular one-hot</td>\n",
       "      <td>non-augmented</td>\n",
       "      <td>0:10:28</td>\n",
       "      <td>1.8297</td>\n",
       "      <td>58.03</td>\n",
       "      <td>0.5312</td>\n",
       "      <td>0.5772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R2P</td>\n",
       "      <td>Genus</td>\n",
       "      <td>regular one-hot</td>\n",
       "      <td>non-augmented</td>\n",
       "      <td>0:24:3</td>\n",
       "      <td>1.4210</td>\n",
       "      <td>66.29</td>\n",
       "      <td>0.6102</td>\n",
       "      <td>0.6607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model    Taxon         Encoding           Data Training time  \\\n",
       "0         CNN    Genus            3-mer  non-augmented        0:3:13   \n",
       "1         CNN    Genus            6-mer  non-augmented        0:5:46   \n",
       "2         CNN   Family            6-mer  non-augmented        0:3:44   \n",
       "3         CNN  Species            6-mer  non-augmented        0:3:45   \n",
       "4      BiLSTM    Genus  regular one-hot  non-augmented       0:44:37   \n",
       "5  ConvBiLSTM    Genus  regular one-hot  non-augmented       0:10:28   \n",
       "6         R2P    Genus  regular one-hot  non-augmented        0:24:3   \n",
       "\n",
       "   Test loss  Test accuracy (%)  F1-score     MCC  \n",
       "0     0.9238              75.94    0.7321  0.7578  \n",
       "1     0.2702              92.48    0.9132  0.9243  \n",
       "2     0.0885              98.09    0.9791  0.9806  \n",
       "3     0.6071              83.14    0.7999  0.8307  \n",
       "4     2.5693              43.40    0.3644  0.4294  \n",
       "5     1.8297              58.03    0.5312  0.5772  \n",
       "6     1.4210              66.29    0.6102  0.6607  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Functions for cleaning the dataframe\n",
    "# to generate a nicer looking table\n",
    "def clean_taxon_name(cell):\n",
    "    if cell == 'spe':\n",
    "        cell = 'Species'\n",
    "    elif cell == 'gen':\n",
    "        cell = 'Genus'\n",
    "    else:\n",
    "        cell = 'Family'\n",
    "    return cell\n",
    "\n",
    "def clean_data_name(cell):\n",
    "    if cell == 'na':\n",
    "        cell = 'non-augmented'\n",
    "    elif cell[0] == 'a':\n",
    "        cell = 'augmented'\n",
    "    else:\n",
    "        cell = 'V-region selected'\n",
    "    return cell\n",
    "\n",
    "def clean_encoding_name(cell):\n",
    "    # for RNN models\n",
    "    if cell == '0':\n",
    "        cell = 'regular one-hot'\n",
    "    elif cell == '1':\n",
    "        cell = 'mutation rate adjusted one-hot'\n",
    "    # for RDP\n",
    "    elif cell == 'X':\n",
    "        cell = '8-mer with parentID'\n",
    "    # for CNN models\n",
    "    else:\n",
    "        cell = cell + '-mer'\n",
    "    return cell\n",
    "\n",
    "def clean_time(sec):\n",
    "    sec = sec % (24 * 3600)\n",
    "    hr = sec // 3600\n",
    "    sec %= 3600\n",
    "    min = sec // 60\n",
    "    sec %= 60\n",
    "    return \"{}:{}:{}\".format(hr, min, sec)\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# making a copy of the dataframe to clean\n",
    "clean_df = df.copy().sort_values('Model')\n",
    "# applying the cleaning functions\n",
    "clean_df['Taxon'] = clean_df['Taxon'].apply(lambda x: clean_taxon_name(x))\n",
    "clean_df['Data'] = clean_df['Data'].apply(lambda x: clean_data_name(x))\n",
    "clean_df['Encoding'] = clean_df['Encoding'].apply(lambda x: clean_encoding_name(x))\n",
    "clean_df['Training time'] = clean_df['Training time'].apply(lambda x: clean_time(x))\n",
    "# rounding the float values\n",
    "clean_df[['Test loss', 'Test accuracy', 'F1-score', 'MCC']] = clean_df[[\n",
    "    'Test loss', 'Test accuracy', 'F1-score', 'MCC'\n",
    "    ]].round(4)\n",
    "# converting test accuracy to percentage\n",
    "clean_df['Test accuracy'] = clean_df['Test accuracy'].apply(lambda x: x*100)\n",
    "clean_df = clean_df.rename(columns={\"Test accuracy\":\"Test accuracy (%)\"})\n",
    "\n",
    "clean_df = clean_df.sort_values('Encoding').reset_index().iloc[: , 1:]\n",
    "clean_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the npy_files containing the training history are loaded. This generates a dictionary of history_dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_files = glob.glob(\"history/*.npy\")\n",
    "\n",
    "histories_dict = {}\n",
    "for np_name in history_files:\n",
    "    # the name is cut to a better looking format\n",
    "    name = np_name.split('\\\\')[1][:-4]\n",
    "    histories_dict[name] = np.load(np_name, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and Discussing the training histories\n",
    "Using the Seaborn package, the function plot_hist() is defined that returns plots for the desired comparison.\n",
    "\n",
    "Using the Pandas package, the function give_score() is defined that returns the scores of the desired comparison.\n",
    "\n",
    "As RDP does not have a history object, it is only compared against the deep learning models with the score table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "sns.set_theme()\n",
    "\n",
    "# list used as keys for which plots to make\n",
    "plots = ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n",
    "# list used as titels to give the subplots\n",
    "titles = ['Training Loss', 'Training Accuracy', 'Validation Loss', 'Validation Accuracy']\n",
    "# list used as labels for y-axis (no unit of measurement for loss)\n",
    "y_labels = ['', 'percentage (%)', '', 'percentage (%)']\n",
    "\n",
    "def plot_hist(hist_dict_lst, epochs = 60, suptitle = 'Comparison_title'):\n",
    "    # Initialise x, y and legend value lists\n",
    "    x_values = np.arange(epochs + 1, dtype=int)\n",
    "    df_lst = [\n",
    "        pd.DataFrame({'Epochs' : x_values}), \n",
    "        pd.DataFrame({'Epochs' : x_values}), \n",
    "        pd.DataFrame({'Epochs' : x_values}), \n",
    "        pd.DataFrame({'Epochs' : x_values})\n",
    "        ]\n",
    "    legend_names = []\n",
    "    # iterating over the histories to be plotted (compared runs)\n",
    "    for key in hist_dict_lst:\n",
    "        hist_dict = histories_dict[key]\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # the names are augmented and minimized \n",
    "        # to show all necessary information in a nice looking way\n",
    "        name_lst = key.split('_')\n",
    "\n",
    "        # making encoding method more informative\n",
    "        if name_lst[0] == 'CNN':\n",
    "            name_lst[2] = name_lst[2] + '-mer encoding'\n",
    "        else:\n",
    "            if name_lst[-1][-1] == 0:\n",
    "                name_lst[2] = 'regular one-hot-encoding'\n",
    "            else:\n",
    "                name_lst[2] = 'mutation rate adjusted one-hot-encoding'\n",
    "        \n",
    "        # making taxon-level more informative\n",
    "        if name_lst[1] == 'spe':\n",
    "            name_lst[1] = 'at Species-level'\n",
    "        elif name_lst[1] == 'gen':\n",
    "            name_lst[1] = 'at Genus-level'\n",
    "        else:\n",
    "            name_lst[1] = 'at Family-level'\n",
    "\n",
    "        # making data name more informative\n",
    "        if name_lst[-1][:2] == 'na':\n",
    "            name_lst[-1] = 'with non-augmented data'\n",
    "        elif name_lst[-1][0] == 'a':\n",
    "            name_lst[-1] = 'with augmented data'\n",
    "        elif name_lst[-1][:2] == 'va':\n",
    "            name_lst[-1] = 'with augmented V-region selected data'\n",
    "        else:\n",
    "            name_lst[-1] = 'with non-augmented V-region selected data'\n",
    "\n",
    "        name = ' '.join(name_lst[:3]) + ' ' + name_lst[-1]\n",
    "        legend_names.append(name)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # iterating over the plots to make\n",
    "        for i, plt_key in enumerate(plots):\n",
    "            plt_values = hist_dict[plt_key].copy()\n",
    "            # adding value for epoch 0 for nicer plots (arbitrary)\n",
    "            # for loss\n",
    "            if i == 0 or i == 2:\n",
    "                plt_values.insert(0, 8)\n",
    "            # for accuracy\n",
    "            if i == 1 or i == 3:\n",
    "                plt_values.insert(0, 0)\n",
    "                # converting accuracy to percentage\n",
    "                plt_values = [plt_value*100 for plt_value in plt_values]\n",
    "            # add value to corresponding dataframe\n",
    "            df_lst[i][key] = plt_values\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # initialise the figure with 4 subplots (2 rows & 2 columns)\n",
    "    fig, ax = plt.subplots(2, 2, figsize = (18, 12))\n",
    "    # iterate over the subplots with the corresponding dataframe\n",
    "    frame = 0\n",
    "    for row in range(2):\n",
    "        for col in range(2):\n",
    "            # melt dataframe for Seaborn to accept it (differentiating run based on hue)\n",
    "            dfm = df_lst[frame].melt('Epochs', var_name = 'Run', value_name = 'values')\n",
    "            plt.rcParams[\"axes.titlesize\"] = 16\n",
    "            sns.lineplot(\n",
    "                data = dfm, x = 'Epochs', y = 'values', hue = 'Run', \n",
    "                ax = ax[row, col], legend = False).set(\n",
    "                    title = titles[frame], \n",
    "                    xlim = (0, epochs), ylim = (0, None), \n",
    "                    ylabel = y_labels[frame])\n",
    "            frame += 1\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # adding a legend and title for the total figure\n",
    "    fig.legend(\n",
    "        labels = legend_names, \n",
    "        title = 'Compared runs (model / encoding / taxon / data)', \n",
    "        loc = 1)\n",
    "    fig.suptitle(\n",
    "        suptitle, \n",
    "        horizontalalignment = 'center', \n",
    "        fontsize = 22,\n",
    "        fontweight = 'bold')\n",
    "\n",
    "def give_score(model, taxon, encoding, data, df = clean_df):\n",
    "    return df.loc[\n",
    "        (df['Model'].isin(model)) & \n",
    "        (df['Data'].isin(data)) &\n",
    "        (df['Encoding'].isin(taxon)) & \n",
    "        (df['Data'].isin(data))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Comparisons\n",
    "The desired comparisons are defined and lists containing the keys for looking up the history_dictionaries are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Lists of history objects for comparing each model\n",
    "# with various encoding methods\n",
    "histories_CNN_encoding_genus = [\n",
    "    'CNN_gen_3_x_train_CNN_na3', \n",
    "    'CNN_gen_5_x_train_CNN_na5',\n",
    "    'CNN_gen_7_x_train_CNN_na7'\n",
    "    ]\n",
    "histories_BiLSTM_encoding_genus = [\n",
    "    'BiLSTM_gen_x_train_RNN_na0', \n",
    "    'BiLSTM_gen_x_train_RNN_na1'\n",
    "    ]\n",
    "histories_ConvBiLSTM_encoding_genus = [\n",
    "    'ConvBiLSTM_gen_x_train_RNN_na0', \n",
    "    'ConvBiLSTM_gen_x_train_RNN_na1'\n",
    "    ]\n",
    "histories_R2P_encoding_genus = [\n",
    "    'R2P_gen_x_train_RNN_na0', \n",
    "    'R2P_gen_x_train_RNN_na1'\n",
    "    ]\n",
    "# -----------------------------------------------------------------\n",
    "# Lists of history objects for comparing each model\n",
    "# at the three taxon levels\n",
    "histories_CNN_taxa = [\n",
    "    'CNN_fam_7_x_train_CNN_na7', \n",
    "    'CNN_gen_7_x_train_CNN_na7', \n",
    "    'CNN_spe_7_x_train_CNN_na7'\n",
    "    ]\n",
    "histories_BiLSTM_taxa = [\n",
    "    'BiLSTM_fam_x_train_RNN_na1', \n",
    "    'BiLSTM_gen_x_train_RNN_na1', \n",
    "    'BiLSTM_spe_x_train_RNN_na1'\n",
    "    ]\n",
    "histories_ConvBiLSTM_taxa = [\n",
    "    'ConvBiLSTM_fam_x_train_RNN_na1', \n",
    "    'ConvBiLSTM_gen_x_train_RNN_na1', \n",
    "    'ConvBiLSTM_spe_x_train_RNN_na1'\n",
    "    ]\n",
    "histories_R2P_taxa = [\n",
    "    'R2P_fam_x_train_RNN_na1', \n",
    "    'R2P_gen_x_train_RNN_na1', \n",
    "    'R2P_spe_x_train_RNN_na1'\n",
    "    ]\n",
    "# -----------------------------------------------------------------\n",
    "# Lists of history objects for comparing each model\n",
    "# with regular/augmented/V-region selected data\n",
    "histories_CNN_data = [\n",
    "    'CNN_gen_7_x_train_CNN_na7', \n",
    "    'CNN_gen_7_x_train_CNN_a7',\n",
    "    'CNN_gen_7_x_train_CNN_v7'\n",
    "    ]\n",
    "histories_BiLSTM_data = [\n",
    "    'BiLSTM_gen_x_train_RNN_na1', \n",
    "    'BiLSTM_gen_x_train_RNN_a1',\n",
    "    'BiLSTM_gen_x_train_RNN_v1'\n",
    "    ]\n",
    "histories_ConvBiLSTM_data = [\n",
    "    'ConvBiLSTM_gen_x_train_RNN_na1', \n",
    "    'ConvBiLSTM_gen_x_train_RNN_a1',\n",
    "    'ConvBiLSTM_gen_x_train_RNN_v1'\n",
    "    ]\n",
    "histories_R2P_data = [\n",
    "    'R2P_gen_x_train_RNN_na1', \n",
    "    'R2P_gen_x_train_RNN_a1',\n",
    "    'R2P_gen_x_train_RNN_v1'\n",
    "    ]\n",
    "# -----------------------------------------------------------------\n",
    "# Lists of history objects for comparing the models with each other\n",
    "#   at species-level on non-augmeted data\n",
    "histories_models_spe = [\n",
    "    'CNN_spe_7_x_train_CNN_na7', \n",
    "    'BiLSTM_spe_x_train_RNN_na1', \n",
    "    'ConvBiLSTM_spe_x_train_RNN_na1', \n",
    "    'R2P_spe_x_train_RNN_na1'\n",
    "    ]\n",
    "#   at genus-level on non-augmeted data\n",
    "histories_models_gen = [\n",
    "    'CNN_gen_7_x_train_CNN_na7', \n",
    "    'BiLSTM_gen_x_train_RNN_na1', \n",
    "    'ConvBiLSTM_gen_x_train_RNN_na1', \n",
    "    'R2P_gen_x_train_RNN_na1'\n",
    "    ]\n",
    "#   at family-level on non-augmeted data\n",
    "histories_models_fam = [\n",
    "    'CNN_fam_7_x_train_CNN_na7', \n",
    "    'BiLSTM_fam_x_train_RNN_na1', \n",
    "    'ConvBiLSTM_fam_x_train_RNN_na1', \n",
    "    'R2P_fam_x_train_RNN_na1'\n",
    "    ]\n",
    "#   at genus-level on augmeted data\n",
    "histories_models_a = [\n",
    "    'CNN_gen_7_x_train_CNN_a7', \n",
    "    'BiLSTM_gen_x_train_RNN_a1', \n",
    "    'ConvBiLSTM_gen_x_train_RNN_a1', \n",
    "    'R2P_gen_x_train_RNN_a1'\n",
    "    ]\n",
    "#   at genus-level on V-region selected data\n",
    "histories_models_v = [\n",
    "    'CNN_gen_7_x_train_CNN_v7', \n",
    "    'BiLSTM_gen_x_train_RNN_v1', \n",
    "    'ConvBiLSTM_gen_x_train_RNN_v1', \n",
    "    'R2P_gen_x_train_RNN_v1'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Comparing the models' results with the various encoding methods\n",
    "**Comparing the CNN model at Genus level with 3-, 5- and 7-mer encoded sequences**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['CNN'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['3-mer', '5-mer', '7-mer'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_CNN_encoding_genus, \n",
    "    suptitle = 'CNN models at Genus-level\\nwith 3-, 5-mer and 7-mer encoded sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the BiLSTM model at Genus level with regular and mutation rate adjusted one-hot-encoded sequences**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['BiLSTM'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['regular one-hot', 'mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_BiLSTM_encoding_genus, \n",
    "    suptitle = 'BiLSTM models at Genus-level\\nwith regular and mutation rate adjusted one-hot-encoded sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the ConvBiLSTM model at Genus level with regular and mutation rate adjusted one-hot-encoded sequences**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['ConvBiLSTM'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['regular one-hot', 'mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_ConvBiLSTM_encoding_genus, \n",
    "    suptitle = 'ConvBiLSTM models at Genus-level\\nwith regular and mutation rate adjusted one-hot-encoded sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the Read2Pheno model at Genus level with regular and mutation rate adjusted one-hot-encoded sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['R2P'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['regular one-hot', 'mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_R2P_encoding_genus, \n",
    "    suptitle = 'Read2Pheno models at Genus-level\\nwith regular and mutation rate adjusted one-hot-encoded sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Comparing the models' results at different taxon levels\n",
    "**Comparing the CNN model at Family, Genus and Species level with 7-mer encoded sequences**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['CNN'], \n",
    "    taxon = ['Family', 'Genus', 'Species'], \n",
    "    encoding = ['7-mer'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_CNN_taxa, \n",
    "    suptitle = 'CNN models at Family, Genus and Species level\\nwith 7-mer encoded sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the BiLSTM model at Family, Genus and Species level with mutation rate adjusted one-hot-encoded sequences**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['BiLSTM'], \n",
    "    taxon = ['Family', 'Genus', 'Species'], \n",
    "    encoding = ['mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_BiLSTM_taxa, \n",
    "    suptitle = 'BiLSTM models at Family, Genus and Species level\\nwith mutation rate adjusted one-hot-encoded sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the ConvBiLSTM model at Family, Genus and Species level with mutation rate adjusted one-hot-encoded sequences**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['ConvBiLSTM'], \n",
    "    taxon = ['Family', 'Genus', 'Species'], \n",
    "    encoding = ['mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_ConvBiLSTM_taxa, \n",
    "    suptitle = 'ConvBiLSTM models at Family, Genus and Species level\\nwith mutation rate adjusted one-hot-encoded sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the Read2Pheno model at Family, Genus and Species level with mutation rate adjusted one-hot-encoded sequences**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['R2P'], \n",
    "    taxon = ['Family', 'Genus', 'Species'], \n",
    "    encoding = ['mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_R2P_taxa, \n",
    "    suptitle = 'Read2Pheno models at Family, Genus and Species level\\nwith mutation rate adjusted one-hot-encoded sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Comparing the models' results with the regular, augmented and V-region selected data\n",
    "**Comparing the CNN model at Genus level with 7-mer encoded sequences on the non-augmented, augmented and V-region selected data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['CNN'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['7-mer'], \n",
    "    data = ['non-augmented', 'augmented', 'V-region selected']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_CNN_data, \n",
    "    suptitle = 'CNN models at Genus level with 7-mer encoded sequences\\non non-augmented, augmented and V-region selected data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the BiLSTM model at Genus level with mutation rate adjusted one-hot-encoded sequences on the non-augmented, augmented and V-region selected data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['BiLSTM'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented', 'augmented', 'V-region selected']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_BiLSTM_data, \n",
    "    suptitle = 'BiLSTM models at Genus level with mutation rate adjusted one-hot-encoded sequences\\non non-augmented, augmented and V-region selected data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the ConvBiLSTM model at Genus level with mutation rate adjusted one-hot-encoded sequences on the non-augmented, augmented and V-region selected data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['ConvBiLSTM'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented', 'augmented', 'V-region selected']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_ConvBiLSTM_data, \n",
    "    suptitle = 'ConvBiLSTM models at Genus level with mutation rate adjusted one-hot-encoded sequences\\non non-augmented, augmented and V-region selected data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the Read2Pheno model at Genus level with mutation rate adjusted one-hot-encoded sequences on the non-augmented, augmented and V-region selected data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['R2P'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented', 'augmented', 'V-region selected']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_R2P_data, \n",
    "    suptitle = 'Read2Pheno models at Genus level with mutation rate adjusted one-hot-encoded sequences\\non non-augmented, augmented and V-region selected data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Comparing the models with each other\n",
    "**Comparing the RDP / CNN / BiLSTM / ConvBiLSTM / Read2Pheno models at species level on the non-augmented data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['RDP', 'CNN', 'BiLSTM', 'ConvBiLSTM', 'R2P'], \n",
    "    taxon = ['Species'], \n",
    "    encoding = ['8-mer', '7-mer', 'mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_models_spe, \n",
    "    suptitle = 'CNN / BiLSTM / ConvBiLSTM / Read2Pheno models at Species level\\non the non-augmented data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the RDP / CNN / BiLSTM / ConvBiLSTM / Read2Pheno models at genus level on the non-augmented data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['RDP', 'CNN', 'BiLSTM', 'ConvBiLSTM', 'R2P'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['8-mer', '7-mer', 'mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_models_gen, \n",
    "    suptitle = 'CNN / BiLSTM / ConvBiLSTM / Read2Pheno models at Genus level\\non the non-augmented data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the RDP / CNN / BiLSTM / ConvBiLSTM / Read2Pheno models at family level on the non-augmented data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['RDP', 'CNN', 'BiLSTM', 'ConvBiLSTM', 'R2P'], \n",
    "    taxon = ['Family'], \n",
    "    encoding = ['8-mer', '7-mer', 'mutation rate adjusted one-hot'], \n",
    "    data = ['non-augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_models_fam, \n",
    "    suptitle = 'CNN / BiLSTM / ConvBiLSTM / Read2Pheno models at Family level\\non the non-augmented data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the CNN / BiLSTM / ConvBiLSTM / Read2Pheno models on the augmented data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['RDP', 'CNN', 'BiLSTM', 'ConvBiLSTM', 'R2P'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['8-mer', '7-mer', 'mutation rate adjusted one-hot'], \n",
    "    data = ['augmented']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_models_a, \n",
    "    suptitle = 'CNN / BiLSTM / ConvBiLSTM / Read2Pheno models at Genus level\\non the augmented data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the CNN / BiLSTM / ConvBiLSTM / Read2Pheno models on the V-region selected data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(give_score(\n",
    "    model = ['RDP', 'CNN', 'BiLSTM', 'ConvBiLSTM', 'R2P'], \n",
    "    taxon = ['Genus'], \n",
    "    encoding = ['8-mer', '7-mer', 'mutation rate adjusted one-hot'], \n",
    "    data = ['V-region selected']\n",
    "    ))\n",
    "\n",
    "plot_hist(\n",
    "    histories_models_v, \n",
    "    suptitle = 'CNN / BiLSTM / ConvBiLSTM / Read2Pheno models at Genus level\\non the V-region selected data')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e755af18eee0034cd700295d0e984a40f53659443b610e850ee570bdbea72f7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
