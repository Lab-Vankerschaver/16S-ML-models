{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative analyses of machine learning-based models\n",
    "This juptyer notebook goes through the compiling, training, testing and visualisation of machine learning models used for bacterial taxonomy classification using the curated 16S rRNA sequence datasets.\n",
    "\n",
    "## Loading modules and training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded!\n"
     ]
    }
   ],
   "source": [
    "# LOADING PACKAGES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from varname import argname, nameof\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, accuracy_score\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Masking, Dot, Add, BatchNormalization\n",
    "from keras.layers import MaxPooling1D, AveragePooling1D, Conv1D, Reshape\n",
    "from keras.layers import TimeDistributed, LSTM, Bidirectional\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "LR, BATCH_SIZE, EPOCHS, MAX_LEN, INPUT_SHAPE_RNN, INPUT_SHAPE_4_MER, INPUT_SHAPE_7_MER = 0.001, 8, 3, 2000, (2000, 4), (625, 1), (78125, 1)\n",
    "\n",
    "print('Packages loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded!\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE NON-AUGMENTED AND AUGMENTED DATASETS\n",
    "train_na = pd.read_csv('df_train_0.csv')\n",
    "val_na = pd.read_csv('df_val_0.csv')\n",
    "test_na = pd.read_csv('df_test_0.csv')\n",
    "# ------------------------------------------------\n",
    "train_a = pd.read_csv('df_train_1.csv')\n",
    "val_a = pd.read_csv('df_val_1.csv')\n",
    "# ------------------------------------------------\n",
    "print('Datasets loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating k-mers\n",
    "For use in the Convolutional Neural Networks (CNN), the sequences are processed into a frequency table of k-mers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# define all possible k-mers\n",
    "alphabet = \"AGTCN\"\n",
    "four_mers = [''.join(chars) for chars in product(*(4*(alphabet,)))]\n",
    "seven_mers = [''.join(chars) for chars in product(*(7*(alphabet,)))]\n",
    "\n",
    "def one_hot_k(sequence, kmers):\n",
    "    k = len(kmers[0])\n",
    "    # define a counter dictionary\n",
    "    kmer_dict = dict.fromkeys(kmers, 0)\n",
    "\n",
    "    # standardize the sequence\n",
    "    # by replacing U with T and all ambiguous bases with N\n",
    "    sequence = sequence.replace('U', 'T').replace('Y', 'N').replace('R', 'N').replace('W', 'N').replace('S', 'N').replace('K', 'N').replace('M', 'N').replace('D', 'N').replace('V', 'N').replace('H', 'N').replace('B', 'N').replace('X', 'N').replace('-', 'N')\n",
    "    # count every k-mer in the sequence\n",
    "    for i in range(0, len(sequence) -k+1):\n",
    "        kmer_dict[sequence[i:i+k]] += 1\n",
    "\n",
    "    # k-mer frequency array from the dictionary values\n",
    "    k_array = np.array(list(kmer_dict.values()))\n",
    "    # normalizing the array by dividing every value with the highest count value\n",
    "    k_array = k_array / np.amax(k_array)\n",
    "    return k_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding the sequences\n",
    "For use in the various Recurrent Neural Networks (RNN), the nucleotide sequences are processed into a one-hot-encoded format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary without consideration of mutation rate (AGTC)\n",
    "one_hot_dict0 = {'A': [1.,0.,0.,0.], 'G':[0.,1.,0.,0.], 'T':[0.,0.,1.,0.], 'U':[0.,0.,1.,0.], 'C':[0.,0.,0.,1.], 'Y':[0.,0.,0.5,0.5], 'R':[0.5,0.5,0.,0.], 'W':[0.5,0.,0.5,0.], 'S':[0.,0.5,0.,0.5], 'K':[0.,0.5,0.5,0.], 'M':[0.5,0.,0.,0.5], 'D':[0.33,0.33,0.33,0.], 'V':[0.33,0.33,0.,0.33], 'H':[0.33,0.,0.33,0.33], 'B':[0.,0.33,0.33,0.33], 'X':[0.25,0.25,0.25,0.25], 'N':[0.25,0.25,0.25,0.25], '-':[0.,0.,0.,0.]}\n",
    "# Dictionary with consideration of mutation rate (AGTC)\n",
    "one_hot_dict1 = {'A': [1.,0.,-0.5,-0.5], 'G':[0.,1.,-0.5,-0.5], 'T':[-0.5,-0.5,1.,0.], 'U':[-0.5,-0.5,1.,0.], 'C':[-0.5,-0.5,0.,1.], 'Y':[-0.5,-0.5,0.5,0.5], 'R':[0.5,0.5,-0.5,-0.5], 'W':[0.5,-0.5,0.5,-0.5], 'S':[-0.5,0.5,-0.5,0.5], 'K':[-0.5,0.5,0.5,-0.5], 'M':[0.5,-0.5,-0.5,0.5], 'D':[0.33,0.33,0.33,-1.], 'V':[0.33,0.33,-1.,0.33], 'H':[0.33,-1.,.33,0.33], 'B':[-1.,0.33,0.33,0.33], 'X':[0.,0.,0.,0.], 'N':[0.,0.,0.,0.], '-':[0.,0.,0.,0.]}\n",
    "\n",
    "def one_hot_seq(sequence, one_hot_dict, MAX_LEN=MAX_LEN):\n",
    "    # padding the sequences to a fixed length\n",
    "\tsequence += '-'*(MAX_LEN - len(sequence))\n",
    "    # generating list of one-hot-lists using the dictionary\n",
    "\tonehot_encoded = [one_hot_dict[nucleotide] for nucleotide in sequence]\n",
    "    # returning the list of lists as a numpy array\n",
    "\treturn np.array(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding the labels\n",
    "For use in the deep learning models, the labels are processed into a one-hot-encoding format. To achieve this, every unique label is first encoded to a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxon_dict(df, taxon):\n",
    "    # listing all unique taxon labels\n",
    "    taxon_list = list(df[taxon].unique())\n",
    "\n",
    "    # generating a dictionary to associate every unique taxon to a number\n",
    "    taxon_dict = dict(zip(taxon_list, range(0, len(taxon_list))))\n",
    "    # and the reversed dictionary as a lookup table\n",
    "    taxon_dict_lookup = {v: k for k, v in taxon_dict.items()}\n",
    "\n",
    "    return taxon_dict, taxon_dict_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the input sequences and labels\n",
    "The encoding methods for nucleotide sequences (x) defined earlier are now applied. The labels (y) are one-hot-encoded using the to_categorical function in keras_utils, thereby, converting the data into the correct format for feeding it to the deep learning models.\n",
    "\n",
    "Every array is saved to reduce memory requirements down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the sequences into k-mer counts and one-hot-encoded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-mer complete\n",
      "7-mer complete\n",
      "CNN sequences complete\n"
     ]
    }
   ],
   "source": [
    "# ENCODING SEQUENCES for the CNN models in 2 processing variations\n",
    "# FOR CNN  |  with 4-mer\n",
    "x_train_CNN_na4 = np.array(train_na['Sequence'].apply(lambda x: one_hot_k(x, four_mers)).tolist())\n",
    "np.save(f'arrays/CNN/{nameof(x_train_CNN_na4)}', x_train_CNN_na4)\n",
    "\n",
    "# x_train_CNN_a4 = np.array(train_a['Sequence'].apply(lambda x: one_hot_k(x, four_mers)).tolist())\n",
    "# np.save(f'arrays/CNN/{nameof(x_train_CNN_a4)}', x_train_CNN_a4)\n",
    "\n",
    "x_test_CNN_na4 = np.array(test_na['Sequence'].apply(lambda x: one_hot_k(x, four_mers)).tolist())\n",
    "np.save(f'arrays/CNN/{nameof(x_test_CNN_na4)}', x_test_CNN_na4)\n",
    "\n",
    "dataval_CNN_na4 = np.array(val_na['Sequence'].apply(lambda x: one_hot_k(x, four_mers)).tolist())\n",
    "np.save(f'arrays/CNN/{nameof(dataval_CNN_na4)}', dataval_CNN_na4)\n",
    "\n",
    "# dataval_CNN_a4 = np.array(val_a['Sequence'].apply(lambda x: one_hot_k(x, four_mers)).tolist())\n",
    "# np.save(f'arrays/CNN/{nameof(dataval_CNN_a4)}', dataval_CNN_a4)\n",
    "print('4-mer complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# FOR CNN  |  with 7-mer\n",
    "x_train_CNN_na7 = np.array(train_na['Sequence'].apply(lambda x: one_hot_k(x, seven_mers)).tolist())\n",
    "np.save(f'arrays/CNN/{nameof(x_train_CNN_na7)}', x_train_CNN_na7)\n",
    "\n",
    "x_train_CNN_a7 = np.array(train_a['Sequence'].apply(lambda x: one_hot_k(x, seven_mers)).tolist())\n",
    "np.save(f'arrays/CNN/{nameof(x_train_CNN_a7)}', x_train_CNN_a7)\n",
    "\n",
    "x_test_CNN_na7 = np.array(test_na['Sequence'].apply(lambda x: one_hot_k(x, seven_mers)).tolist())\n",
    "np.save(f'arrays/CNN/{nameof(x_test_CNN_na7)}', x_test_CNN_na7)\n",
    "\n",
    "dataval_CNN_na7 = np.array(val_na['Sequence'].apply(lambda x: one_hot_k(x, seven_mers)).tolist())\n",
    "np.save(f'arrays/CNN/{nameof(dataval_CNN_na7)}', dataval_CNN_na7)\n",
    "\n",
    "dataval_CNN_a7 = np.array(val_a['Sequence'].apply(lambda x: one_hot_k(x, seven_mers)).tolist())\n",
    "np.save(f'arrays/CNN/{nameof(dataval_CNN_a7)}', dataval_CNN_a7)\n",
    "print('7-mer complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "print('CNN sequences complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular one-hot-encoding complete\n",
      "Mutations rate adjusted one-hot-encoding complete\n",
      "RNN sequences complete\n"
     ]
    }
   ],
   "source": [
    "# ENCODING SEQUENCES for the RNN models in 2 processing variations\n",
    "# FOR RNN  |  with regular one-hot-encoding\n",
    "x_train_RNN_na0 = np.array(train_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0)).tolist())\n",
    "np.save(f'arrays/RNN/{nameof(x_train_RNN_na0)}', x_train_RNN_na0)\n",
    "\n",
    "# x_train_RNN_a0 = np.array(train_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0)).tolist())\n",
    "# np.save(f'arrays/RNN/{nameof(x_train_RNN_a0)}', x_train_RNN_a0)\n",
    "\n",
    "x_test_RNN_na0 = np.array(test_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0)).tolist())\n",
    "np.save(f'arrays/RNN/{nameof(x_test_RNN_na0)}', x_test_RNN_na0)\n",
    "\n",
    "dataval_RNN_na0 = np.array(val_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0)).tolist())\n",
    "np.save(f'arrays/RNN/{nameof(dataval_RNN_na0)}', dataval_RNN_na0)\n",
    "\n",
    "# dataval_RNN_a0 = np.array(val_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict0)).tolist())\n",
    "# np.save(f'arrays/RNN/{nameof(dataval_RNN_a0)}', dataval_RNN_a0)\n",
    "print('Regular one-hot-encoding complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# FOR RNN  |  with matation rate adjusted one-hot-encoding\n",
    "x_train_RNN_na1 = np.array(train_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1)).tolist())\n",
    "np.save(f'arrays/RNN/{nameof(x_train_RNN_na1)}', x_train_RNN_na1)\n",
    "\n",
    "x_train_RNN_a1 = np.array(train_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1)).tolist())\n",
    "np.save(f'arrays/RNN/{nameof(x_train_RNN_a1)}', x_train_RNN_a1)\n",
    "\n",
    "x_test_RNN_na1 = np.array(test_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1)).tolist())\n",
    "np.save(f'arrays/RNN/{nameof(x_test_RNN_na1)}', x_test_RNN_na1)\n",
    "\n",
    "dataval_RNN_na1 = np.array(val_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1)).tolist())\n",
    "np.save(f'arrays/RNN/{nameof(dataval_RNN_na1)}', dataval_RNN_na1)\n",
    "\n",
    "dataval_RNN_a1 = np.array(val_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict1)).tolist())\n",
    "np.save(f'arrays/RNN/{nameof(dataval_RNN_a1)}', dataval_RNN_a1)\n",
    "print('Mutations rate adjusted one-hot-encoding complete')\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "print('RNN sequences complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels into a one-hot-encoded format at Family level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels complete\n",
      "Family train/test/val arrays generated\n",
      "The number of unique family labels: 349\n"
     ]
    }
   ],
   "source": [
    "taxon = 'Family'\n",
    "taxon_dict = get_taxon_dict(test_na, taxon)[0]\n",
    "\n",
    "# Associate every entry's label in the df to a number \n",
    "#   using the dictionary & one-hot encode the numerical labels\n",
    "y_train_fam_na = to_categorical(y=train_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/family/{nameof(y_train_fam_na)}', y_train_fam_na)\n",
    "\n",
    "# y_train_fam_a = to_categorical(y=train_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "# np.save(f'arrays/family/{nameof(y_train_fam_a)}', y_train_fam_a)\n",
    "\n",
    "y_test_fam_na = to_categorical(y=test_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/family/{nameof(y_test_fam_na)}', y_test_fam_na)\n",
    "\n",
    "labelsval_fam_na = to_categorical(y=val_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/family/{nameof(labelsval_fam_na)}', labelsval_fam_na)\n",
    "\n",
    "# labelsval_fam_a = to_categorical(y=val_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "# np.save(f'arrays/family/{nameof(labelsval_fam_a)}', labelsval_fam_a)\n",
    "print('Family label arrays generated')\n",
    "# ------------------------------------------------------------------------------------\n",
    "fam_count = train_na[taxon].nunique()\n",
    "print(f'The number of unique family labels: {fam_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels into a one-hot-encoded format at Genus level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genus train/test/val arrays generated\n",
      "The number of unique genus labels: 954\n"
     ]
    }
   ],
   "source": [
    "taxon = 'Genus'\n",
    "taxon_dict = get_taxon_dict(test_na, taxon)[0]\n",
    "\n",
    "y_train_gen_na = to_categorical(y=train_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/genus/{nameof(y_train_gen_na)}', y_train_gen_na)\n",
    "\n",
    "y_train_gen_a = to_categorical(y=train_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/genus/{nameof(y_train_gen_a)}', y_train_gen_a)\n",
    "\n",
    "y_test_gen_na = to_categorical(y=test_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/genus/{nameof(y_test_gen_na)}', y_test_gen_na)\n",
    "\n",
    "labelsval_gen_na = to_categorical(y=val_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/genus/{nameof(labelsval_gen_na)}', labelsval_gen_na)\n",
    "\n",
    "labelsval_gen_a = to_categorical(y=val_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/genus/{nameof(labelsval_gen_a)}', labelsval_gen_a)\n",
    "print('Genus label arrays generated')\n",
    "# ------------------------------------------------------------------------------------\n",
    "gen_count = train_na[taxon].nunique()\n",
    "print(f'The number of unique genus labels: {gen_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels into a one-hot-encoded format at Species level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species train/test/val arrays generated\n",
      "The number of unique species labels: 1569\n"
     ]
    }
   ],
   "source": [
    "taxon = 'Species'\n",
    "taxon_dict = get_taxon_dict(test_na, taxon)[0]\n",
    "\n",
    "y_train_spe_na = to_categorical(y=train_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/species/{nameof(y_train_spe_na)}', y_train_spe_na)\n",
    "\n",
    "# y_train_spe_a = to_categorical(y=train_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "# np.save(f'arrays/species/{nameof(y_train_spe_a)}', y_train_spe_a)\n",
    "\n",
    "y_test_spe_na = to_categorical(y=test_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/species/{nameof(y_test_spe_na)}', y_test_spe_na)\n",
    "\n",
    "labelsval_spe_na = to_categorical(y=val_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "np.save(f'arrays/species/{nameof(labelsval_spe_na)}', labelsval_spe_na)\n",
    "\n",
    "# labelsval_spe_a = to_categorical(y=val_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "# np.save(f'arrays/species/{nameof(labelsval_spe_a)}', labelsval_spe_a)\n",
    "print('Species label arrays generated')\n",
    "# ------------------------------------------------------------------------------------\n",
    "spe_count = train_na[taxon].nunique()\n",
    "print(f'The number of unique species labels: {spe_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the generated training/validation/testing arrays\n",
    "When seperating the processing and training/evaluation scripts, due to memory limits, the saved arrays can be loaded using this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMOUNT OF UNIQUE LABELS AT EACH TAXON LEVEL\n",
    "fam_count, gen_count, spe_count = 349, 954, 1569\n",
    "\n",
    "# LOADING ENCODED SEQUENCES for the CNN models in 2 processing variations\n",
    "# FOR CNN  |  with 4-mer\n",
    "x_train_CNN_na4 = np.load(f'arrays/CNN/x_train_CNN_na4.npy')\n",
    "# x_train_CNN_a4 = np.load(f'arrays/CNN/x_train_CNN_a4.npy')\n",
    "x_test_CNN_na4 = np.load(f'arrays/CNN/x_test_CNN_na4.npy')\n",
    "dataval_CNN_na4 = np.load(f'arrays/CNN/dataval_CNN_na4.npy')\n",
    "# dataval_CNN_a4 = np.load(f'arrays/CNN/dataval_CNN_a4.npy')\n",
    "print('4-mers LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# FOR CNN  |  with 7-mer\n",
    "x_train_CNN_na7 = np.load(f'arrays/CNN/x_train_CNN_na7.npy')\n",
    "x_train_CNN_a7 = np.load(f'arrays/CNN/x_train_CNN_a7.npy')\n",
    "x_test_CNN_na7 = np.load(f'arrays/CNN/x_test_CNN_na7.npy')\n",
    "dataval_CNN_na7 = np.load(f'arrays/CNN/dataval_CNN_na7.npy')\n",
    "dataval_CNN_a7 = np.load(f'arrays/CNN/dataval_CNN_a7.npy')\n",
    "print('7-mers LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "print('CNN sequences LOADED')\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# LOADING ENCODED SEQUENCES for the RNN models in 2 processing variations\n",
    "# FOR RNN  |  with regular one-hot-encoding\n",
    "x_train_RNN_na0 = np.load(f'arrays/CNN/x_train_RNN_na0.npy')\n",
    "# x_train_RNN_a0 = np.load(f'arrays/CNN/x_train_RNN_a0.npy')\n",
    "x_test_RNN_na0 = np.load(f'arrays/CNN/x_test_RNN_na0.npy')\n",
    "dataval_RNN_na0 = np.load(f'arrays/CNN/dataval_RNN_na0.npy')\n",
    "# dataval_RNN_a0 = np.load(f'arrays/CNN/dataval_RNN_a0.npy')\n",
    "print('Regular one-hot-encoded sequences LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# FOR RNN  |  with matation rate adjusted one-hot-encoding\n",
    "x_train_RNN_na1 = np.load(f'arrays/CNN/x_train_RNN_na1.npy')\n",
    "x_train_RNN_a1 = np.load(f'arrays/CNN/x_train_RNN_a1.npy')\n",
    "x_test_RNN_na1 = np.load(f'arrays/CNN/x_test_RNN_na1.npy')\n",
    "dataval_RNN_na1 = np.load(f'arrays/CNN/dataval_RNN_na1.npy')\n",
    "dataval_RNN_a1 = np.load(f'arrays/CNN/dataval_RNN_a1.npy')\n",
    "print('Mutation rate adjusted one-hot-encoded sequences LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "print('RNN sequences LOADED')\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# LOADING one-hot encoded labels at each taxon level\n",
    "# -----------------------------------------------------------------------------\n",
    "# LABELS AT FAMILY LEVEL\n",
    "y_train_fam_na = np.load(f'arrays/CNN/y_train_fam_na.npy')\n",
    "# y_train_fam_a = np.load(f'arrays/CNN/y_train_fam_a.npy')\n",
    "y_test_fam_na = np.load(f'arrays/CNN/y_test_fam_na.npy')\n",
    "labelsval_fam_na = np.load(f'arrays/CNN/labelsval_fam_na.npy')\n",
    "# labelsval_fam_a = np.load(f'arrays/CNN/labelsval_fam_a.npy')\n",
    "print('Family label arrays LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# LABELS AT GENUS LEVEL\n",
    "y_train_gen_na = np.load(f'arrays/CNN/y_train_gen_na.npy')\n",
    "y_train_gen_a = np.load(f'arrays/CNN/y_train_gen_a.npy')\n",
    "y_test_gen_na = np.load(f'arrays/CNN/y_test_gen_na.npy')\n",
    "labelsval_gen_na = np.load(f'arrays/CNN/labelsval_gen_na.npy')\n",
    "labelsval_gen_a = np.load(f'arrays/CNN/labelsval_gen_a.npy')\n",
    "print('Genus label arrays LOADED')\n",
    "# -----------------------------------------------------------------------------\n",
    "# LABELS AT SPECIES LEVEL\n",
    "y_train_spe_na = np.load(f'arrays/CNN/y_train_spe_na.npy')\n",
    "# y_train_spe_a = np.load(f'arrays/CNN/y_train_spe_a.npy')\n",
    "y_test_spe_na = np.load(f'arrays/CNN/y_test_spe_na.npy')\n",
    "labelsval_spe_na = np.load(f'arrays/CNN/labelsval_spe_na.npy')\n",
    "# labelsval_spe_a = np.load(f'arrays/CNN/labelsval_spe_a.npy')\n",
    "print('Species label arrays LOADED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the network architectures\n",
    "What follows are a set of functions for creating the deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "def make_CNNmodel(input_shape, out_len, name='CNN'):\n",
    "    CNNmodel = keras.Sequential(\n",
    "        [\n",
    "            Reshape(target_shape = input_shape, input_shape = input_shape[:-1]),\n",
    "            Conv1D(5, 5, padding='valid', input_shape=input_shape),\n",
    "            Activation('relu'),\n",
    "            MaxPooling1D(pool_size=2, padding='valid'),\n",
    "\n",
    "            Conv1D(10, 5, padding='valid'),\n",
    "            Activation('relu'),\n",
    "            MaxPooling1D(pool_size=2, padding='valid'),\n",
    "\n",
    "            Flatten(),\n",
    "            Dense(500),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(out_len, activation='softmax')\n",
    "        ], \n",
    "        name = name\n",
    "    )\n",
    "    return CNNmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bidirectional Long-Short Term Memory Neural Network (BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "def make_BiLSTMmodel(out_len, INPUT_SHAPE=INPUT_SHAPE_RNN, name='BiLSTM'):\n",
    "    BiLSTMmodel = keras.Sequential(\n",
    "        [\n",
    "            Masking(mask_value=0., input_shape=INPUT_SHAPE_RNN),\n",
    "            Bidirectional(LSTM(128, return_sequences=True), merge_mode='sum'),\n",
    "            Dropout(0.5),\n",
    "            AveragePooling1D(4),\n",
    "            Bidirectional(LSTM(128), merge_mode='sum'),\n",
    "            Dropout(0.5),\n",
    "            Dense((out_len), activation='softmax'),\n",
    "        ],\n",
    "        name=name\n",
    "    )\n",
    "    return BiLSTMmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convolutional BiLSTM Neural Network (ConvBiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvBiLSTM\n",
    "def make_ConvBiLSTMmodel(out_len, INPUT_SHAPE=INPUT_SHAPE_RNN, name='ConvBiLSTM'):\n",
    "    ConvBiLSTMmodel = keras.Sequential(\n",
    "        [\n",
    "            Masking(mask_value=0., input_shape=INPUT_SHAPE),\n",
    "                        \n",
    "            Conv1D(128, 3),\n",
    "            AveragePooling1D(),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Conv1D(128, 3),\n",
    "            AveragePooling1D(),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Conv1D(128, 3, use_bias=True),\n",
    "            AveragePooling1D(),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            Bidirectional(LSTM(128, activation='tanh'), merge_mode='sum'),\n",
    "            Dropout(0.2),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(out_len, activation='softmax')\n",
    "        ], \n",
    "        name = name\n",
    "    )\n",
    "    return ConvBiLSTMmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Attention-based ConvBiLSTM (Read2Pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read2Pheno\n",
    "## Conv & Res net layers\n",
    "CONV_NET_nr, RES_NET_nr, NET_filters, NET_window = 2, 1, 64, 2\n",
    "## extra Dropout layer (after Res block)\n",
    "DROP_r, POOL_s = 0.2, 2\n",
    "## BiLSTM layer\n",
    "LSTM_nodes = 128\n",
    "## attention Layers\n",
    "ATT_layers,vATT_nodes = 1, 128\n",
    "## fully connected layers\n",
    "FC_layers, FC_nodes, FC_drop = 1, 128, 0.3\n",
    "\n",
    "#####################################################################################################\n",
    "# BLOCK FUNCTIONS\n",
    "def conv_net_block(X, n_cnn_filters=256, cnn_window=9, block_name='convblock'):\n",
    "    '''\n",
    "    convolutional block with a 1D convolutional layer, a batch norm layer followed by a relu activation.\n",
    "    parameters:\n",
    "        n_cnn_filters: number of output channels\n",
    "        cnn_window: window size of the 1D convolutional layer\n",
    "    '''\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "def res_net_block(X, n_cnn_filters=256, cnn_window=9, block_name='resblock'):\n",
    "    '''\n",
    "    residual net block accomplished by a few convolutional blocks.\n",
    "    parameters:\n",
    "        n_cnn_filters: number of output channels\n",
    "        cnn_window: window size of the 1D convolutional layer\n",
    "    '''\n",
    "    X_identity = X\n",
    "    # cnn0\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    # cnn1\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    # cnn2\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Add()([X, X_identity])\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "def attention_layer(H_lstm, n_layer, n_node, block_name='att'):\n",
    "    '''\n",
    "    feedforward attention layer accomplished by time distributed dense layers.\n",
    "    parameters:\n",
    "        n_layer: number of hidden layers\n",
    "        n_node: number of hidden nodes\n",
    "    '''\n",
    "    H_emb = H_lstm\n",
    "    for i in range(n_layer):\n",
    "        H_lstm = TimeDistributed(Dense(n_node, activation=\"tanh\"))(H_lstm)\n",
    "    M = TimeDistributed(Dense(1, activation=\"linear\"))(H_lstm)\n",
    "    alpha = keras.layers.Softmax(axis=1)(M)\n",
    "    r_emb = Dot(axes = 1)([alpha, H_emb])\n",
    "    r_emb = Flatten()(r_emb)\n",
    "    return r_emb\n",
    "\n",
    "def fully_connected(r_emb, n_layer, n_node, drop_out_rate=0.5, block_name='fc'):\n",
    "    '''\n",
    "    fully_connected layer consists of a few dense layers.\n",
    "    parameters:\n",
    "        n_layer: number of hidden layers\n",
    "        n_node: number of hidden nodes\n",
    "        drop_out_rate: dropout rate to prevent the model from overfitting\n",
    "    '''\n",
    "    for i in range(n_layer):\n",
    "        r_emb = Dense(n_node, activation=\"relu\")(r_emb)\n",
    "    r_emb = Dropout(drop_out_rate)(r_emb) \n",
    "    return r_emb\n",
    "    \n",
    "#####################################################################################################\n",
    "# TOTAL MODEL FUNCTION\n",
    "\n",
    "def make_R2Pmodel(out_len, INPUT_SHAPE=INPUT_SHAPE_RNN, name='Read2Pheno'):\n",
    "    X = Input(shape=INPUT_SHAPE)\n",
    "    X_mask = Masking(mask_value=0.)(X)\n",
    "\n",
    "    ## CONV Layers\n",
    "    X_cnn = X_mask\n",
    "    # conv_net\n",
    "    for i in range(CONV_NET_nr):\n",
    "        X_cnn = conv_net_block(X_cnn, n_cnn_filters=NET_filters, cnn_window=NET_window)\n",
    "    # res_net\n",
    "    for i in range(RES_NET_nr):\n",
    "        X_cnn = res_net_block(X_cnn, n_cnn_filters=NET_filters, cnn_window=NET_window)\n",
    "\n",
    "    ## Extra Pooling layer and Dropout\n",
    "    X_pool = AveragePooling1D(pool_size=POOL_s)(X_cnn)\n",
    "    X_drop = Dropout(DROP_r)(X_pool)\n",
    "\n",
    "    ## RNN Layers\n",
    "    H_lstm = Bidirectional(LSTM(LSTM_nodes, return_sequences=True), merge_mode='sum')(X_drop)\n",
    "    H_lstm = Activation('tanh')(H_lstm)\n",
    "\n",
    "    ## ATT Layers\n",
    "    r_emb = attention_layer(H_lstm, n_layer=ATT_layers, n_node=ATT_nodes, block_name = 'att')\n",
    "        \n",
    "    # Fully connected layers\n",
    "    r_emb = fully_connected(r_emb, n_layer=FC_layers, n_node=FC_nodes, drop_out_rate=FC_drop, block_name = 'fc')\n",
    "\n",
    "    # Compile model\n",
    "    out = Dense(out_len, activation='softmax', name='final_dense')(r_emb)\n",
    "    R2Pmodel = Model(inputs = X, outputs = out, name = name)\n",
    "    \n",
    "    return R2Pmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Deep Learning models\n",
    "The models are generated based on input and output shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CNN\n",
    "The CNN models are created, tailored to the different input (k-mer) and output (taxon) shapes. The input is determined by the k-mer used and the output shape is determined by the amount of unique taxon labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Family\n",
    "CNN_fam_4 = make_CNNmodel(input_shape=INPUT_SHAPE_4_MER, out_len=fam_count, name='CNN_Family-level_4-mer') # with 4-mer\n",
    "CNN_fam_7 = make_CNNmodel(input_shape=INPUT_SHAPE_7_MER, out_len=fam_count, name='CNN_Family-level_7-mer') # with 7-mer\n",
    "# for Genus\n",
    "CNN_gen_4 = make_CNNmodel(input_shape=INPUT_SHAPE_4_MER, out_len=gen_count, name='CNN_Genus-level_4-mer')\n",
    "CNN_gen_7 = make_CNNmodel(input_shape=INPUT_SHAPE_7_MER, out_len=gen_count, name='CNN_Genus-level_7-mer')\n",
    "# for Species\n",
    "CNN_spe_4 = make_CNNmodel(input_shape=INPUT_SHAPE_4_MER, out_len=spe_count, name='CNN_Species-level_4-mer')\n",
    "CNN_spe_7 = make_CNNmodel(input_shape=INPUT_SHAPE_7_MER, out_len=spe_count, name='CNN_Species-level_7-mer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RNN\n",
    "The RNN models are created, tailored to the different output shapes. The output shape is determined by the amount of unique taxon labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Family\n",
    "BiLSTM_fam = make_BiLSTMmodel(out_len=fam_count, name='BiLSTM_Family-level')\n",
    "ConvBiLSTM_fam = make_ConvBiLSTMmodel(out_len=fam_count, name='ConvBiLSTM_Family-level')\n",
    "R2P_fam = make_R2Pmodel(out_len=fam_count, name='Read2Pheno_Family-level')\n",
    "# for Genus\n",
    "BiLSTM_gen = make_BiLSTMmodel(out_len=gen_count, name='BiLSTM_Genus-level')\n",
    "ConvBiLSTM_gen = make_ConvBiLSTMmodel(out_len=gen_count, name='ConvBiLSTM_Genus-level')\n",
    "R2P_gen = make_R2Pmodel(out_len=gen_count, name='Read2Pheno_Genus-level')\n",
    "# for Species\n",
    "BiLSTM_spe = make_BiLSTMmodel(out_len=spe_count, name='BiLSTM_Species-level')\n",
    "ConvBiLSTM_spe = make_ConvBiLSTMmodel(out_len=spe_count, name='ConvBiLSTM_Species-level')\n",
    "R2P_spe = make_R2Pmodel(out_len=spe_count, name='Read2Pheno_Species-level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling, Training and Evaluating the Deep Learning models\n",
    "\n",
    "For each model:\n",
    "- a Weights and Biases run is initiated\n",
    "- the model is compiled and a summary is printed\n",
    "- the model is trained and training-time is measured\n",
    "- the model is evaluated by calculating the test-loss and -accuracy, the F1 score and the MCC score\n",
    "- the training history and metrics are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_data, train_labels, validation_data, validation_labels, test_data, test_labels):\n",
    "    wandb.init(project = 'Test training2', entity = 'bachelorprojectgroup9', name=model.name)\n",
    "\n",
    "    print (f'Loading {model.name} model...')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=LR), metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    print (f'Fitting {model.name} model...')\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_data, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data = (validation_data, validation_labels), callbacks=[WandbCallback()])\n",
    "    time_taken = round(time.time() - start_time)\n",
    "    np.save(f'history/{model.name}_history.npy', history.history)\n",
    "    \n",
    "    print (f'Evaluating {model.name} model...')\n",
    "    test_labels_arg = np.argmax(test_labels, axis=1)\n",
    "    test_predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "    loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "\n",
    "    # F1-score: harmonic mean of the precision and recall\n",
    "    #   score from 0 to 1\n",
    "    f1 = f1_score(y_true=test_labels_arg, y_pred=test_predictions, average='weighted')\n",
    "    # Matthews correlation coefficient: coefficient of +1 represents a perfect prediction,\n",
    "    #   0 an average random prediction and -1 an inverse prediction\n",
    "    mcc = matthews_corrcoef(y_true=test_labels_arg, y_pred=test_predictions)\n",
    "\n",
    "    score_dict = pd.DataFrame({'Model/run' : model.name, 'Data' : argname('train_data'), 'Training time' : time_taken, 'Test loss' : loss, 'Test accuracy' : accuracy, 'F1-score' : f1, 'MCC' : mcc}, index=[0])\n",
    "    print(score_dict)\n",
    "    score_dict.to_csv(f'scores/{model.name}_evaluation', index=False)\n",
    "\n",
    "    wandb.finish()\n",
    "    return history, score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING CNN MODELS\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# running the model at genus level with both k-mers\n",
    "CNN_gen_4_history, CNN_gen_4_score_dict = train_and_evaluate_model(CNN_gen_4, x_train_CNN_na4, y_train_gen_na, dataval_CNN_na4, labelsval_gen_na, x_test_CNN_na4, y_test_gen_na)\n",
    "CNN_gen_7_history, CNN_gen_7_score_dict = train_and_evaluate_model(CNN_gen_7, x_train_CNN_na7, y_train_gen_na, dataval_CNN_na7, labelsval_gen_na, x_test_CNN_na7, y_test_gen_na)\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# running the 7-mer model at family and species level\n",
    "CNN_fam_7_history, CNN_fam_7_score_dict = train_and_evaluate_model(CNN_fam_7, x_train_CNN_na7, y_train_fam_na, dataval_CNN_na7, labelsval_fam_na, x_test_CNN_na7, y_test_fam_na)\n",
    "CNN_spe_7_history, CNN_spe_7_score_dict = train_and_evaluate_model(CNN_spe_7, x_train_CNN_na7, y_train_spe_na, dataval_CNN_na7, labelsval_spe_na, x_test_CNN_na7, y_test_spe_na)\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# running the 7-mer model at genus level on the augmented data\n",
    "CNN_gen_7a_history, CNN_gen_7a_score_dict = train_and_evaluate_model(CNN_gen_7, x_train_CNN_a7, y_train_gen_a, dataval_CNN_a7, labelsval_gen_na, x_test_CNN_na7, y_test_gen_na)\n",
    "\n",
    "###################################################################################################################################################################################################\n",
    "\n",
    "# RUNNING RNN MODELS\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# running the models at genus level with both one-hot-encodings\n",
    "BiLSTM_gen_0_history, BiLSTM_gen_0_score_dict = train_and_evaluate_model(BiLSTM_gen, x_train_RNN_na0, y_train_gen_na, dataval_RNN_na0, labelsval_gen_na, x_test_RNN_na0, y_test_gen_na)\n",
    "ConvBiLSTM_gen_0_history, ConvBiLSTM_gen_0_score_dict = train_and_evaluate_model(ConvBiLSTM_gen, x_train_RNN_na0, y_train_gen_na, dataval_RNN_na0, labelsval_gen_na, x_test_RNN_na0, y_test_gen_na)\n",
    "R2P_gen_0_history, R2P_gen_0_score_dict = train_and_evaluate_model(R2P_gen, x_train_RNN_na0, y_train_gen_na, dataval_RNN_na0, labelsval_gen_na, x_test_RNN_na0, y_test_gen_na)\n",
    "\n",
    "BiLSTM_gen_history, BiLSTM_gen_score_dict = train_and_evaluate_model(BiLSTM_gen, x_train_RNN_na1, y_train_gen_na, dataval_RNN_na1, labelsval_gen_na, x_test_RNN_na1, y_test_gen_na)\n",
    "ConvBiLSTM_gen_history, ConvBiLSTM_gen_score_dict = train_and_evaluate_model(ConvBiLSTM_gen, x_train_RNN_na1, y_train_gen_na, dataval_RNN_na1, labelsval_gen_na, x_test_RNN_na1, y_test_gen_na)\n",
    "R2P_gen_1_history, R2P_gen_1_score_dict = train_and_evaluate_model(R2P_gen, x_train_RNN_na1, y_train_gen_na, dataval_RNN_na1, labelsval_gen_na, x_test_RNN_na1, y_test_gen_na)\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# running the models at family and species level with the mutation rate adjusted one-hot-encoding\n",
    "BiLSTM_fam_history, BiLSTM_fam_score_dict = train_and_evaluate_model(BiLSTM_fam, x_train_RNN_na1, y_train_fam_na, dataval_RNN_na1, labelsval_fam_na, x_test_RNN_na1, y_test_fam_na)\n",
    "ConvBiLSTM_fam_history, ConvBiLSTM_fam_score_dict = train_and_evaluate_model(ConvBiLSTM_fam, x_train_RNN_na1, y_train_fam_na, dataval_RNN_na1, labelsval_fam_na, x_test_RNN_na1, y_test_fam_na)\n",
    "R2P_fam_history, R2P_fam_score_dict = train_and_evaluate_model(R2P_fam, x_train_RNN_na1, y_train_fam_na, dataval_RNN_na1, labelsval_fam_na, x_test_RNN_na1, y_test_fam_na)\n",
    "\n",
    "BiLSTM_spe_history, BiLSTM_spe_score_dict = train_and_evaluate_model(BiLSTM_spe, x_train_RNN_na1, y_train_spe_na, dataval_RNN_na1, labelsval_spe_na, x_test_RNN_na1, y_test_spe_na)\n",
    "ConvBiLSTM_spe_history, ConvBiLSTM_spe_score_dict = train_and_evaluate_model(ConvBiLSTM_spe, x_train_RNN_na1, y_train_spe_na, dataval_RNN_na1, labelsval_spe_na, x_test_RNN_na1, y_test_spe_na)\n",
    "R2P_spe_history, R2P_spe_score_dict = train_and_evaluate_model(R2P_spe, x_train_RNN_na1, y_train_spe_na, dataval_RNN_na1, labelsval_spe_na, x_test_RNN_na1, y_test_spe_na)\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# running the models at family level with the mutation rate adjusted one-hot-encoding on the augmented data\n",
    "BiLSTM_gen_a_history, BiLSTM_gen_a_score_dict = train_and_evaluate_model(BiLSTM_gen, x_train_RNN_a1, y_train_gen_a, dataval_RNN_a1, labelsval_gen_a, x_test_RNN_na1, y_test_gen_na)\n",
    "ConvBiLSTM_gen_a_history, ConvBiLSTM_gen_a_score_dict = train_and_evaluate_model(ConvBiLSTM_gen, x_train_RNN_a1, y_train_gen_a, dataval_RNN_a1, labelsval_gen_a, x_test_RNN_na1, y_test_gen_na)\n",
    "R2P_gen_a_history, R2P_gen_a_score_dict = train_and_evaluate_model(R2P_gen, x_train_RNN_a1, y_train_gen_a, dataval_RNN_a1, labelsval_gen_a, x_test_RNN_na1, y_test_gen_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ribosomal Database Project (RDP)\n",
    "Train and validation dataset are merged into one, this is becuase the RDP machine learning model does not require validation steps during training. The databases are converted to ready4train taxonomy and fasta files to be fed into the RDP Classifier.\n",
    "\n",
    "When running cells, new files will be saved in \"RDPfiles\" directory at the same folder of this jupyter notebook is located. Directory name can be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting variables\n",
    "Training and classification is done with the whole taxon hierarchy, from kingdom to species, but a specific level can be chosen during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global RDPfiles\n",
    "\n",
    "#variables\n",
    "RDPfiles = \"RDPfiles\"\n",
    "classifier_loc = \"rdptools/classifier.jar\"\n",
    "confidence_score = 0.8\n",
    "level = 'genus' #ranks = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing functions\n",
    "**Lineage2taxTrain** converts tab separated taxonomy text files into the ready4train_taxonomy.txt file. This text file contains the hierarchical taxonomy information in the following format: tax ID | taxon name | parent taxid | depth | rank.\n",
    "- Tax ID is the index of the rank in the taxonomy file\n",
    "- Taxon name is the name for the taxonomic rank\n",
    "- Parent taxid is the tax ID of the rank above the current rank\n",
    "- Depth is the depth of the rank. Depth 0 is always root. The kingdom rank has depth of 1\n",
    "- Rank is the taxonomic ranks\n",
    "\n",
    "**AddFullLineage** generates the ready4train_seqs.fasta file. It has structure similar to the sequence fasta file, but semicolon separated taxonomy is added next to sequence ID.\n",
    "\n",
    "\n",
    "**RDPoutput2score** generates the accuracy, F1 score and MCC using output.txt and test_taxonomy.txt. Output.txt is generated when classifying test_sequences.txt with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineage2taxTrain(raw_taxons):\n",
    "    taxons_list = raw_taxons.strip().split('\\n')\n",
    "    header = taxons_list[0].split('\\t')[1:] # headers = list of ranks\n",
    "    hash = {} # taxon name-id map\n",
    "    ranks = {} # column number-rank map\n",
    "    lineages = [] # list of unique lineages\n",
    "\n",
    "    with open(\"{}/ready4train_taxonomy.txt\".format(RDPfiles), \"w\") as f:\n",
    "        # initiate root rank taxon id map\n",
    "        hash = {\"Root\":0}\n",
    "        for i in range(len(header)):\n",
    "            name = header[i]\n",
    "            ranks[i] = name\n",
    "\n",
    "        # root rank info\n",
    "        root = ['0', 'Root', '-1', '0', 'rootrank']\n",
    "        f.write(\"*\".join(root) +  '\\n')\n",
    "\n",
    "        ID = 0\n",
    "        for line in taxons_list[1:]:\n",
    "            cols = line.strip().split('\\t')[1:]\n",
    "            # iterate each column\n",
    "            for i in range(len(cols)):\n",
    "                name = []\n",
    "                for node in cols[:i + 1]:\n",
    "                    node = node.strip()\n",
    "                    if not node in ('-', ''):\n",
    "                        name.append(node)\n",
    "\n",
    "                pName = \";\".join(name[:-1])\n",
    "                if not name in lineages:\n",
    "                    lineages.append(name)\n",
    "\n",
    "                depth = len(name)\n",
    "                name = ';'.join(name)\n",
    "                if name in hash.keys():\n",
    "                    # already seen this lineage\n",
    "                    continue\n",
    "                try:\n",
    "                    rank = ranks[i]\n",
    "                except KeyError:\n",
    "                    print (cols)\n",
    "                    sys.exit()\n",
    "\n",
    "                if i == 0:\n",
    "                    pName = 'Root'\n",
    "                # parent taxid\n",
    "                pID = hash[pName]\n",
    "                ID += 1\n",
    "                # add name-id to the map\n",
    "                hash[name] = ID\n",
    "                out = ['%s'%ID, name.split(';')[-1], '%s'%pID, '%s'%depth, rank]\n",
    "                f.write(\"*\".join(out) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def addFullLineage(raw_taxons, raw_seqs):\n",
    "    # lineage map\n",
    "    hash = {}\n",
    "    taxonomy_list = raw_taxons.strip().split('\\n')\n",
    "\n",
    "    for line in taxonomy_list[1:]:\n",
    "        line = line.strip()\n",
    "        cols = line.strip().split('\\t')\n",
    "        lineage = ['Root']\n",
    "\n",
    "        for node in cols[1:]:\n",
    "            node = node.strip()\n",
    "            if not (node == '-' or node == ''):\n",
    "                lineage.append(node)\n",
    "\n",
    "        ID = cols[0]\n",
    "        lineage = ';'.join(lineage).strip()\n",
    "        hash[ID] = lineage\n",
    "\n",
    "    sequence_list = raw_seqs.strip().split('\\n')\n",
    "    with open(\"{}/ready4train_seqs.fasta\".format(RDPfiles), \"w\") as f:\n",
    "        for line in sequence_list:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                continue\n",
    "            if line[0] == '>':\n",
    "                ID = line.strip().split()[0].replace('>', '')\n",
    "                lineage = hash[ID]\n",
    "                f.write('>' + ID + '\\t' + lineage + '\\n')\n",
    "            else:\n",
    "                f.write(line.strip() + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def RDPoutput2score(pred_file, true_file, level, cf):\n",
    "    taxon_list = []\n",
    "    ranks = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "    level = ranks.index(level)\n",
    "\n",
    "    pred = pd.read_csv(pred_file, sep=\"\\t\", header=None)\n",
    "    pred.drop(pred.columns[1:level+5+2*level],  axis = 'columns', inplace=True)\n",
    "    pred.drop(pred.columns[4:], axis = 'columns', inplace=True)\n",
    "    \n",
    "    pred_dict = {}\n",
    "    for index, row in pred.iterrows():\n",
    "        row = row.tolist()\n",
    "        if row[1] not in taxon_list:\n",
    "            taxon_list += [row[1]]\n",
    "        if float(row[3]) >= cf:\n",
    "            pred_dict[row[0]] = row[1]\n",
    "\n",
    "    true = pd.read_csv(true_file, sep=\"\\t\", header=None)\n",
    "    true_dict = {}\n",
    "    for index, row in true.iterrows():\n",
    "        true_dict[row[0]] = row[level+1]\n",
    "        if row[level+1] not in taxon_list:\n",
    "            taxon_list += [row[level+1]]\n",
    "\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for i in pred_dict.keys():\n",
    "        y_pred.append(taxon_list.index(pred_dict[i]))\n",
    "        y_true.append(taxon_list.index(true_dict[i]))\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') # works well if both y_true_decode and y_pred are list\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    print(\"accuracy: {}\\nF1-score: {}\\nMCC-score: {}\".format(acc, f1, mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "In this cell, a new directory is made to save output.\n",
    "\n",
    "- Train and validation sets are merged into one training set.\n",
    "- raw_seqs + raw_taxons -> ready4train files (input for training the RDP model)\n",
    "- test dataset -> test_sequcnes.fasta + test_taxonomy.txt (used in evaluation of new trained models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing for RDP completed\n"
     ]
    }
   ],
   "source": [
    "# main_RDP.py\n",
    "os.system(\"mkdir {}\".format(RDPfiles))\n",
    "\n",
    "# merge train and validation dataframes into one.\n",
    "train = pd.concat([train_na, val_na], ignore_index=True)\n",
    "# train = pd.concat([train_a, val_a], ignore_index=True)\n",
    "test = test_na\n",
    "\n",
    "# convert train and test dataframe into tab separated taxonomy and sequence string\n",
    "# taxnomy file is converted to a tab separated string\n",
    "# sequence file is converted to fasta format with sequence ID and sequence\n",
    "raw_seqs = ''\n",
    "raw_taxons = 'SeqId Kingdom\tPhylum\tClass\tOrder\tFamily\tGenus\tSpecies' + '\\n'\n",
    "for index, row in train.iterrows():\n",
    "    taxons = row.tolist()\n",
    "    raw_seqs += '>' + taxons[0] + '\\n' + taxons[-1] + '\\n'\n",
    "    raw_taxons += '\\t'.join(taxons[:-1]) + '\\n'\n",
    "\n",
    "# convert test dataframe into text and fasta files to be utilized by RDP\n",
    "# taxnomy file is converted to a tab separated text file\n",
    "# sequence file is converted to fasta format\n",
    "with open(\"{}/test_sequences.fasta\".format(RDPfiles), \"w\") as seq_f, open(\"{}/test_taxonomy.txt\".format(RDPfiles), \"w\") as tax_f:\n",
    "    for index, row in test.iterrows():\n",
    "        taxons = row.tolist()\n",
    "        seq_f.write('>' + taxons[0] + '\\n' + taxons[-1] + '\\n')\n",
    "        tax_f.write('\\t'.join(taxons[:-1]) + '\\n')\n",
    "    seq_f.close()\n",
    "    tax_f.close()\n",
    "\n",
    "# convert raw taxonomy and sequence files to ready4rdp trainable files\n",
    "lineage2taxTrain(raw_taxons)\n",
    "addFullLineage(raw_taxons, raw_seqs)\n",
    "\n",
    "print(\"Data preprocessing for RDP completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is performed with the train() function. The models is saved in training_files directory with four weight files. New file, rRNAClassifier.properties, are necessary for bridging these files and the RDP Classifier. Classification is performed with the classify() function. Option -o leads RDP Classifier to use the newly generated training models.\n",
    "\n",
    "Using RDPoutput2score, the accuracy, F1 score and MCC are calculated. These values will be used to compare with the deep learning models. User can choose specific level of taxon to evaluate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDP training-time: 85 seconds\n",
      "accuracy: 0.9724941724941725\n",
      "F1-score: 0.9726149488062451\n",
      "MCC-score: 0.9723559927778965\n"
     ]
    }
   ],
   "source": [
    "# Training the RDP classifier\n",
    "start_time = time.time()\n",
    "os.system(\"java -Xmx10g -jar {} train -o {}/training_files -s {}/ready4train_seqs.fasta -t {}/ready4train_taxonomy.txt\".format(classifier_loc, RDPfiles, RDPfiles, RDPfiles))\n",
    "with open(\"{}/training_files/rRNAClassifier.properties\".format(RDPfiles), \"w\") as f:\n",
    "    f.write(\"bergeyTree=bergeyTrainingTree.xml\" + '\\n' + \"probabilityList=genus_wordConditionalProbList.txt\" + '\\n' + \"probabilityIndex=wordConditionalProbIndexArr.txt\" + '\\n' + \"wordPrior=logWordPrior.txt\" + '\\n' + \"classifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.5, May 2012 \")\n",
    "    f.close()\n",
    "time_taken = round(time.time() - start_time)\n",
    "print(\"RDP training-time: {} seconds\".format(time_taken))\n",
    "\n",
    "# Testing the RDP classifier\n",
    "os.system(\"java -Xmx10g -jar {} classify -t {}/training_files/rRNAClassifier.properties  -o {}/output.txt {}/test_sequences.fasta\".format(classifier_loc, RDPfiles, RDPfiles, RDPfiles))\n",
    "\n",
    "# Evaluating the RDP classifier\n",
    "RDPoutput2score(\"{}/output.txt\".format(RDPfiles), \"{}/test_taxonomy.txt\".format(RDPfiles), level, confidence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e755af18eee0034cd700295d0e984a40f53659443b610e850ee570bdbea72f7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
