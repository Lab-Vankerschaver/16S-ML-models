{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative analyses of machine learning models\n",
    "This juptyer notebook goes through the compiling, training, testing and visualization of all the machine learning models.\n",
    "\n",
    "## Loading modules and training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded!\n"
     ]
    }
   ],
   "source": [
    "# LOADING PACKAGES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Masking, Dot, Add, BatchNormalization\n",
    "from keras.layers import MaxPooling1D, AveragePooling1D, Conv1D\n",
    "from keras.layers import TimeDistributed, LSTM, Bidirectional\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "LR, BATCH_SIZE, EPOCHS, MAX_LEN, INPUT_SHAPE_RNN, INPUT_SHAPE_4_MER, INPUT_SHAPE_7_MER = 0.001, 8, 3, 2000, (2000, 4), (625, 1), (78125, 1)\n",
    "\n",
    "print('Packages loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded!\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE NON-AUGMENTED AND AUGMENTED DATASETS\n",
    "\n",
    "train_na = pd.read_csv('df_train_0.csv')\n",
    "val_na = pd.read_csv('df_val_0.csv')\n",
    "test_na = pd.read_csv('df_test_0.csv')\n",
    "\n",
    "train_a = pd.read_csv('df_train_1.csv')\n",
    "val_a = pd.read_csv('df_val_1.csv')\n",
    "\n",
    "print('Datasets loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating k-mers\n",
    "For use in the convolutional neural network (CNN), the sequences are processed into a frequency table of k-mers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import Counter\n",
    "\n",
    "def one_hot_k(sequence, k=4):\n",
    "    # define all possible k-mers\n",
    "    alphabet = \"AGTCN\"\n",
    "    kmers = [''.join(chars) for chars in product(*(k*(alphabet,)))]\n",
    "\n",
    "    # standardize the sequence\n",
    "    # by replacing U with T and all ambiguous bases with N\n",
    "    sequence = sequence.replace('U', 'T').replace('Y', 'N').replace('R', 'N').replace('W', 'N').replace('S', 'N').replace('K', 'N').replace('M', 'N').replace('D', 'N').replace('V', 'N').replace('H', 'N').replace('B', 'N').replace('X', 'N').replace('-', 'N')\n",
    "    # split sequences into k-mers\n",
    "    kmer_content = []\n",
    "    for i in range(0, len(sequence) -k+1):\n",
    "        kmer_content.append(sequence[i:i+k])\n",
    "    # count k-mers\n",
    "    counts = Counter(kmer_content)\n",
    "    kmer_dict = {}\n",
    "    for kmer in kmers:\n",
    "        kmer_dict[kmer] = counts[kmer]\n",
    "    # k-mer frequency array from the dictionary values\n",
    "    k_array = np.array(list(kmer_dict.values()))\n",
    "    # normalizing the array\n",
    "    k_array = k_array / np.amax(k_array)\n",
    "    return k_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding the sequences\n",
    "For use in the various recurrent neural networks (RNN), the nucleotide sequences are processed into a one-hot-encoding format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_seq(sequence, MAX_LEN=MAX_LEN, mutation_r=True):\n",
    "\tif not mutation_r:\n",
    "\t\t# Dictionary without care for mutation rate (AGTC)\n",
    "\t\tone_hot_dict = {'A': [1.,0.,0.,0.], 'G':[0.,1.,0.,0.], 'T':[0.,0.,1.,0.], 'U':[0.,0.,1.,0.], 'C':[0.,0.,0.,1.], 'Y':[0.,0.,0.5,0.5], 'R':[0.5,0.5,0.,0.], 'W':[0.5,0.,0.5,0.], 'S':[0.,0.5,0.,0.5], 'K':[0.,0.5,0.5,0.], 'M':[0.5,0.,0.,0.5], 'D':[0.33,0.33,0.33,0.], 'V':[0.33,0.33,0.,0.33], 'H':[0.33,0.,0.33,0.33], 'B':[0.,0.33,0.33,0.33], 'X':[0.25,0.25,0.25,0.25], 'N':[0.25,0.25,0.25,0.25], '-':[0.,0.,0.,0.]}\n",
    "\tif mutation_r:\n",
    "\t\t# Dictionary with care for mutation rate (AGTC)\n",
    "\t\tone_hot_dict = {'A': [1.,0.,-0.5,-0.5], 'G':[0.,1.,-0.5,-0.5], 'T':[-0.5,-0.5,1.,0.], 'U':[-0.5,-0.5,1.,0.], 'C':[-0.5,-0.5,0.,1.], 'Y':[-0.5,-0.5,0.5,0.5], 'R':[0.5,0.5,-0.5,-0.5], 'W':[0.5,-0.5,0.5,-0.5], 'S':[-0.5,0.5,-0.5,0.5], 'K':[-0.5,0.5,0.5,-0.5], 'M':[0.5,-0.5,-0.5,0.5], 'D':[0.33,0.33,0.33,-1.], 'V':[0.33,0.33,-1.,0.33], 'H':[0.33,-1.,.33,0.33], 'B':[-1.,0.33,0.33,0.33], 'X':[0.,0.,0.,0.], 'N':[0.,0.,0.,0.], '-':[0.,0.,0.,0.]}\n",
    "\n",
    "    # padding the sequences to a fixed length\n",
    "\tsequence += '-'*(MAX_LEN - len(sequence))\n",
    "    # generating an empty list and adding one-hot-lists using the dictionary\n",
    "\tonehot_encoded = list()\n",
    "\tfor nucleotide in sequence:\n",
    "\t\tonehot_encoded.append(one_hot_dict[nucleotide])\n",
    "    # returning the list of lists and a numpy array\n",
    "\treturn np.array(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding the labels\n",
    "For use in the deep learning models, the labels are processed into a one-hot-encoding format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxon_dict(df, taxon):\n",
    "    # listing all unique taxon labels\n",
    "    taxon_list = list(df[taxon].unique())\n",
    "\n",
    "    # generating a dictionary to associate every unique taxon to a number\n",
    "    taxon_dict = dict(zip(taxon_list, range(0, len(taxon_list))))\n",
    "    # and the reversed dictionary as a lookup table\n",
    "    taxon_dict_lookup = {v: k for k, v in taxon_dict.items()}\n",
    "\n",
    "    return taxon_dict, taxon_dict_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the x/y train, validation and test sets\n",
    "The processing methods are now applied to generate the different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At family level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels complete\n",
      "1\n",
      "2\n",
      "CNN complete\n",
      "RNN complete\n",
      "Family train/test/val arrays generated\n",
      "Amount of family labels: 349\n"
     ]
    }
   ],
   "source": [
    "# Generating one-hot encoded train, validation and test data at the Family level\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "# ONE-HOT-ENCODING LABELS\n",
    "taxon = 'Family'\n",
    "taxon_dict = get_taxon_dict(test_na, taxon)[0]\n",
    "# Associate every entry's label in the df to a number using the dictionary & one-hot encode the numerical labels\n",
    "y_train_fam_na = to_categorical(y=train_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "# y_train_fam_a = to_categorical(y=train_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "y_test_fam_na = to_categorical(y=test_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "labelsval_fam_na = to_categorical(y=val_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "# labelsval_fam_a = to_categorical(y=val_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "print('Labels complete')\n",
    "\n",
    "######################################################################################################################\n",
    "# ENCODING SEQUENCES for both model types in 2 processing variations (this is the same for every taxon level)\n",
    "# FOR CNN  |  with 4- and 7-mer\n",
    "x_train_CNN_na4 = np.array(train_na['Sequence'].apply(lambda x: one_hot_k(x, k=4)).tolist())\n",
    "# x_train_CNN_a4 = np.array(train_a['Sequence'].apply(lambda x: one_hot_k(x, k=4)).tolist())\n",
    "x_test_CNN_na4 = np.array(test_na['Sequence'].apply(lambda x: one_hot_k(x, k=4)).tolist())\n",
    "dataval_CNN_na4 = np.array(val_na['Sequence'].apply(lambda x: one_hot_k(x, k=4)).tolist())\n",
    "# dataval_CNN_a4 = np.array(val_a['Sequence'].apply(lambda x: one_hot_k(x, k=4)).tolist())\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "x_train_CNN_na7 = np.array(train_na['Sequence'].apply(lambda x: one_hot_k(x, k=7)).tolist())\n",
    "x_train_CNN_a7 = np.array(train_a['Sequence'].apply(lambda x: one_hot_k(x, k=7)).tolist())\n",
    "x_test_CNN_na7 = np.array(test_na['Sequence'].apply(lambda x: one_hot_k(x, k=7)).tolist())\n",
    "dataval_CNN_na7 = np.array(val_na['Sequence'].apply(lambda x: one_hot_k(x, k=7)).tolist())\n",
    "dataval_CNN_a7 = np.array(val_a['Sequence'].apply(lambda x: one_hot_k(x, k=7)).tolist())\n",
    "print('CNN sequences complete')\n",
    "\n",
    "# FOR RNN  |  with regular and matation rate adjusted one-hot-encoding\n",
    "x_train_RNN_na0 = np.array(train_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=False)).tolist())\n",
    "# x_train_RNN_a0 = np.array(train_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=False)).tolist())\n",
    "x_test_RNN_na0 = np.array(test_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=False)).tolist())\n",
    "dataval_RNN_na0 = np.array(val_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=False)).tolist())\n",
    "# dataval_RNN_a0 = np.array(val_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=False)).tolist())\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "x_train_RNN_na1 = np.array(train_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=True)).tolist())\n",
    "x_train_RNN_a1 = np.array(train_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=True)).tolist())\n",
    "x_test_RNN_na1 = np.array(test_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=True)).tolist())\n",
    "dataval_RNN_na1 = np.array(val_na['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=True)).tolist())\n",
    "dataval_RNN_a1 = np.array(val_a['Sequence'].apply(lambda x: one_hot_seq(x, one_hot_dict=True)).tolist())\n",
    "print('RNN sequences complete')\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "print('Family train/test/val arrays generated')\n",
    "\n",
    "fam_count = train_na[taxon].nunique()\n",
    "print(f'Amount of family labels: {fam_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At genus level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating one-hot encoded train, validation and test data at the Genus level\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "# ONE-HOT-ENCODING LABELS\n",
    "taxon = 'Genus'\n",
    "taxon_dict = get_taxon_dict(test_na, taxon)[0]\n",
    "\n",
    "y_train_gen_na = to_categorical(y=train_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "y_train_gen_a = to_categorical(y=train_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "y_test_gen_na = to_categorical(y=test_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "labelsval_gen_na = to_categorical(y=val_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "labelsval_gen_a = to_categorical(y=val_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "print('Genus train/test/val arrays generated')\n",
    "\n",
    "gen_count = train_na[taxon].nunique()\n",
    "print(f'Amount of genus labels: {gen_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At species level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating one-hot encoded train, validation and test data at the Genus level\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "# ONE-HOT-ENCODING LABELS\n",
    "taxon = 'Species'\n",
    "taxon_dict = get_taxon_dict(test_na, taxon)[0]\n",
    "\n",
    "y_train_spe_na = to_categorical(y=train_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "# y_train_spe_a = to_categorical(y=train_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "y_test_spe_na = to_categorical(y=test_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "labelsval_spe_na = to_categorical(y=val_na[taxon].map(taxon_dict).astype(np.float32))\n",
    "# labelsval_spe_a = to_categorical(y=val_a[taxon].map(taxon_dict).astype(np.float32))\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "print('Species train/test/val arrays generated')\n",
    "\n",
    "spe_count = train_na[taxon].nunique()\n",
    "print(f'Amount of species labels: {spe_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the network architectures\n",
    "What follows are a set of functions for creating the deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) with k-mer based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "def make_CNNmodel(input_shape, out_len, name='CNN'):\n",
    "    CNNmodel = keras.Sequential(\n",
    "        [\n",
    "            Conv1D(5, 5, padding='valid', input_shape=input_shape),\n",
    "            Activation('relu'),\n",
    "            MaxPooling1D(pool_size=2, padding='valid'),\n",
    "\n",
    "            Conv1D(10, 5, padding='valid'),\n",
    "            Activation('relu'),\n",
    "            MaxPooling1D(pool_size=2, padding='valid'),\n",
    "\n",
    "            Flatten(),\n",
    "            Dense(500),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(out_len, activation='softmax')\n",
    "        ], \n",
    "        name = name\n",
    "    )\n",
    "    return CNNmodel\n",
    "\n",
    "# in_shape = (len(kmers), 1)\n",
    "# out_shape = fam_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilateral Long-Short Term Memory Neural Network (BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "def make_BiLSTMmodel(out_len, INPUT_SHAPE=INPUT_SHAPE_RNN, name='BiLSTM'):\n",
    "    BiLSTMmodel = keras.Sequential(\n",
    "        [\n",
    "            Masking(mask_value=0., input_shape=INPUT_SHAPE_RNN),\n",
    "            Bidirectional(LSTM(128, return_sequences=True), merge_mode='sum'),\n",
    "            Dropout(0.5),\n",
    "            AveragePooling1D(4),\n",
    "            Bidirectional(LSTM(128), merge_mode='sum'),\n",
    "            Dropout(0.5),\n",
    "            Dense((out_len), activation='softmax'),\n",
    "        ],\n",
    "        name=name\n",
    "    )\n",
    "    return BiLSTMmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional BiLSTM Neural Network (ConvBiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvBiLSTM\n",
    "def make_ConvBiLSTMmodel(out_len, INPUT_SHAPE=INPUT_SHAPE_RNN, name='ConvBiLSTM'):\n",
    "    ConvBiLSTMmodel = keras.Sequential(\n",
    "        [\n",
    "            Masking(mask_value=0., input_shape=INPUT_SHAPE),\n",
    "                        \n",
    "            Conv1D(128, 3),\n",
    "            AveragePooling1D(),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Conv1D(128, 3),\n",
    "            AveragePooling1D(),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Conv1D(128, 3, use_bias=True),\n",
    "            AveragePooling1D(),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            Bidirectional(LSTM(128, activation='tanh'), merge_mode='sum'),\n",
    "            Dropout(0.2),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(out_len, activation='softmax')\n",
    "        ], \n",
    "        name = name\n",
    "    )\n",
    "    return ConvBiLSTMmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention-based ConvBiLSTM (Read2Pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read2Pheno\n",
    "## Conv & Res net layers\n",
    "CONV_NET_nr = 2\n",
    "RES_NET_nr = 1\n",
    "NET_filters = 64\n",
    "NET_window = 2\n",
    "## extra Dropout layers (1 after Res block)\n",
    "DROP_r = 0.2\n",
    "POOL_s = 2\n",
    "## BiLSTM layer\n",
    "LSTM_nodes = 128\n",
    "## attention Layers\n",
    "ATT_layers = 1\n",
    "ATT_nodes = 128\n",
    "## fully connected layers\n",
    "FC_layers = 1\n",
    "FC_nodes = 128\n",
    "FC_drop = 0.3\n",
    "\n",
    "#####################################################################################################\n",
    "# BLOCK FUNCTIONS\n",
    "def conv_net_block(X, n_cnn_filters=256, cnn_window=9, block_name='convblock'):\n",
    "    '''\n",
    "    convolutional block with a 1D convolutional layer, a batch norm layer followed by a relu activation.\n",
    "    parameters:\n",
    "        n_cnn_filters: number of output channels\n",
    "        cnn_window: window size of the 1D convolutional layer\n",
    "    '''\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "def res_net_block(X, n_cnn_filters=256, cnn_window=9, block_name='resblock'):\n",
    "    '''\n",
    "    residual net block accomplished by a few convolutional blocks.\n",
    "    parameters:\n",
    "        n_cnn_filters: number of output channels\n",
    "        cnn_window: window size of the 1D convolutional layer\n",
    "    '''\n",
    "    X_identity = X\n",
    "    # cnn0\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    # cnn1\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    # cnn2\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Add()([X, X_identity])\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "def attention_layer(H_lstm, n_layer, n_node, block_name='att'):\n",
    "    '''\n",
    "    feedforward attention layer accomplished by time distributed dense layers.\n",
    "    parameters:\n",
    "        n_layer: number of hidden layers\n",
    "        n_node: number of hidden nodes\n",
    "    '''\n",
    "    H_emb = H_lstm\n",
    "    for i in range(n_layer):\n",
    "        H_lstm = TimeDistributed(Dense(n_node, activation=\"tanh\"))(H_lstm)\n",
    "    M = TimeDistributed(Dense(1, activation=\"linear\"))(H_lstm)\n",
    "    alpha = keras.layers.Softmax(axis=1)(M)\n",
    "    r_emb = Dot(axes = 1)([alpha, H_emb])\n",
    "    r_emb = Flatten()(r_emb)\n",
    "    return r_emb\n",
    "\n",
    "def fully_connected(r_emb, n_layer, n_node, drop_out_rate=0.5, block_name='fc'):\n",
    "    '''\n",
    "    fully_connected layer consists of a few dense layers.\n",
    "    parameters:\n",
    "        n_layer: number of hidden layers\n",
    "        n_node: number of hidden nodes\n",
    "        drop_out_rate: dropout rate to prevent the model from overfitting\n",
    "    '''\n",
    "    for i in range(n_layer):\n",
    "        r_emb = Dense(n_node, activation=\"relu\")(r_emb)\n",
    "    r_emb = Dropout(drop_out_rate)(r_emb) \n",
    "    return r_emb\n",
    "    \n",
    "#####################################################################################################\n",
    "# TOTAL MODEL FUNCTION\n",
    "\n",
    "def make_R2Pmodel(out_len, INPUT_SHAPE=INPUT_SHAPE_RNN, name='Read2Pheno'):\n",
    "    X = Input(shape=INPUT_SHAPE)\n",
    "    X_mask = Masking(mask_value=0.)(X)\n",
    "\n",
    "    ## CONV Layers\n",
    "    X_cnn = X_mask\n",
    "    # conv_net\n",
    "    for i in range(CONV_NET_nr):\n",
    "        X_cnn = conv_net_block(X_cnn, n_cnn_filters=NET_filters, cnn_window=NET_window)\n",
    "    # res_net\n",
    "    for i in range(RES_NET_nr):\n",
    "        X_cnn = res_net_block(X_cnn, n_cnn_filters=NET_filters, cnn_window=NET_window)\n",
    "\n",
    "    ## Extra Pooling layer and Dropout\n",
    "    X_pool = AveragePooling1D(pool_size=POOL_s)(X_cnn)\n",
    "    X_drop = Dropout(DROP_r)(X_pool)\n",
    "\n",
    "    ## RNN Layers\n",
    "    H_lstm = Bidirectional(LSTM(LSTM_nodes, return_sequences=True), merge_mode='sum')(X_drop)\n",
    "    H_lstm = Activation('tanh')(H_lstm)\n",
    "\n",
    "    ## ATT Layers\n",
    "    r_emb = attention_layer(H_lstm, n_layer=ATT_layers, n_node=ATT_nodes, block_name = 'att')\n",
    "        \n",
    "    # Fully connected layers\n",
    "    r_emb = fully_connected(r_emb, n_layer=FC_layers, n_node=FC_nodes, drop_out_rate=FC_drop, block_name = 'fc')\n",
    "\n",
    "    # Compile model\n",
    "    out = Dense(out_len, activation='softmax', name='final_dense')(r_emb)\n",
    "    R2Pmodel = Model(inputs = X, outputs = out, name = name)\n",
    "    \n",
    "    return R2Pmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "The CNN models are created, tailored to the different input and output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Family\n",
    "CNN_fam_4 = make_CNNmodel(input_shape=INPUT_SHAPE_4_MER, out_len=fam_count, name='CNN_Family-level_4-mer') # with 4-mer\n",
    "CNN_fam_7 = make_CNNmodel(input_shape=INPUT_SHAPE_7_MER, out_len=fam_count, name='CNN_Family-level_7-mer') # with 7-mer\n",
    "# for Genus\n",
    "CNN_gen_4 = make_CNNmodel(input_shape=INPUT_SHAPE_4_MER, out_len=gen_count, name='CNN_Genus-level_4-mer')\n",
    "CNN_gen_7 = make_CNNmodel(input_shape=INPUT_SHAPE_7_MER, out_len=gen_count, name='CNN_Genus-level_7-mer')\n",
    "# for Species\n",
    "CNN_spe_4 = make_CNNmodel(input_shape=INPUT_SHAPE_4_MER, out_len=spe_count, name='CNN_Species-level_4-mer')\n",
    "CNN_spe_7 = make_CNNmodel(input_shape=INPUT_SHAPE_7_MER, out_len=spe_count, name='CNN_Species-level_7-mer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "The RNN models are created, tailored to the different output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_BiLSTMmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3580/819268278.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# for Family\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mBiLSTM_fam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_BiLSTMmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfam_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'BiLSTM_Family-level'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mConvBiLSTM_fam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_ConvBiLSTMmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfam_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ConvBiLSTM_Family-level'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mR2P_fam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_R2Pmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfam_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Read2Pheno_Family-level'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# for Genus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_BiLSTMmodel' is not defined"
     ]
    }
   ],
   "source": [
    "# for Family\n",
    "BiLSTM_fam = make_BiLSTMmodel(output_len=fam_count, name='BiLSTM_Family-level')\n",
    "ConvBiLSTM_fam = make_ConvBiLSTMmodel(output_len=fam_count, name='ConvBiLSTM_Family-level')\n",
    "R2P_fam = make_R2Pmodel(output_len=fam_count, name='Read2Pheno_Family-level')\n",
    "# for Genus\n",
    "BiLSTM_gen = make_BiLSTMmodel(output_len=gen_count, name='BiLSTM_Genus-level')\n",
    "ConvBiLSTM_gen = make_ConvBiLSTMmodel(output_len=gen_count, name='ConvBiLSTM_Genus-level')\n",
    "R2P_gen = make_R2Pmodel(output_len=gen_count, name='Read2Pheno_Genus-level')\n",
    "# for Species\n",
    "BiLSTM_spe = make_BiLSTMmodel(output_len=spe_count, name='BiLSTM_Species-level')\n",
    "ConvBiLSTM_spe = make_ConvBiLSTMmodel(output_len=spe_count, name='ConvBiLSTM_Species-level')\n",
    "R2P_spe = make_R2Pmodel(output_len=spe_count, name='Read2Pheno_Species-level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling, Training and Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_data, train_labels, validation_data, validation_labels, test_data, test_labels):\n",
    "    wandb.init(project = 'Test training', entity = 'bachelorprojectgroup9', name=model.name)\n",
    "    # wandb.config = {'learning_rate':0.001, 'epochs':3, 'batch_size':8}\n",
    "\n",
    "    print (f'Loading {model.name} model...')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=LR), metrics=['accuracy'])\n",
    "    print(model)\n",
    "\n",
    "    print (f'Fitting {model.name} model...')\n",
    "    history = model.fit(train_data, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data = (validation_data, validation_labels), callbacks=[WandbCallback()])\n",
    "    np.save(f'{model.name}_history', history.history)\n",
    "    \n",
    "    print (f'Evaluating {model.name} model...')\n",
    "    predictions = model.predict_classes(test_data)\n",
    "    score, accuracy = model.evaluate(test_data, test_labels)\n",
    "\n",
    "    # F1-score: harmonic mean of the precision and recall\n",
    "    #   score from 0 to 1\n",
    "    f1 = f1_score(y_true=test_labels, y_pred=predictions, average='weighted')\n",
    "    # Matthews correlation coefficient: coefficient of +1 represents a perfect prediction,\n",
    "    #   0 an average random prediction and -1 an inverse prediction\n",
    "    mcc = matthews_corrcoef(y_true=test_labels, y_pred=predictions)\n",
    "\n",
    "    score_dict = pd.DataFrame({'Model/run' : model.name, 'Test loss' : score, 'Test accuracy' : accuracy, 'F1-score' : f1, 'MCC' : mcc})\n",
    "    print(score_dict)\n",
    "    score_dict.to_csv(f'{model.name}_evaluation', index=False)\n",
    "\n",
    "    return history, score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmodels = [CNN_fam_4, CNN_fam_7, CNN_gen_4, CNN_gen_7, CNN_spe_4, CNN_spe_7]\n",
    "RNNmodels= [BiLSTM_fam, ConvBiLSTM_fam, R2P_fam, BiLSTM_gen, ConvBiLSTM_gen, R2P_gen, BiLSTM_spe, ConvBiLSTM_spe, R2P_spe]\n",
    "\n",
    "# RUNNING CNN MODELS\n",
    "# running the model at genus level with both k-mers\n",
    "CNN_gen_4_history, CNN_gen_4_score_dict = train_and_evaluate_model(CNN_gen_4, x_train_CNN_na4, y_train_gen_na, dataval_CNN_na4, labelsval_gen_na, x_test_CNN_na4, y_test_gen_na)\n",
    "CNN_gen_7_history, CNN_gen_7_score_dict = train_and_evaluate_model(CNN_gen_7, x_train_CNN_na7, y_train_gen_na, dataval_CNN_na7, labelsval_gen_na, x_test_CNN_na7, y_test_gen_na)\n",
    "\n",
    "# running the 7-mer model at family and species level\n",
    "CNN_fam_7_history, CNN_fam_7_score_dict = train_and_evaluate_model(CNN_fam_7, x_train_CNN_na7, y_train_fam_na, dataval_CNN_na7, labelsval_fam_na, x_test_CNN_na7, y_test_fam_na)\n",
    "CNN_spe_7_history, CNN_spe_7_score_dict = train_and_evaluate_model(CNN_spe_7, x_train_CNN_na7, y_train_spe_na, dataval_CNN_na7, labelsval_spe_na, x_test_CNN_na7, y_test_spe_na)\n",
    "\n",
    "# running the 7-mer model at genus level on the augmented data\n",
    "CNN_gen_7a_history, CNN_gen_7a_score_dict = train_and_evaluate_model(CNN_gen_7, x_train_CNN_a7, y_train_gen_a, dataval_CNN_a7, labelsval_gen_na, x_test_CNN_na7, y_test_gen_na)\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "# RUNNING RNN MODELS\n",
    "# running the models at genus level with both one-hot-encodings\n",
    "BiLSTM0_gen_history, BiLSTM0_gen_score_dict = train_and_evaluate_model(BiLSTM_gen, x_train_RNN_na0, y_train_gen_na, dataval_RNN_na0, labelsval_gen_na, x_test_RNN_na0, y_test_gen_na)\n",
    "ConvBiLSTM0_gen_history, ConvBiLSTM0_gen_score_dict = train_and_evaluate_model(ConvBiLSTM_gen, x_train_RNN_na0, y_train_gen_na, dataval_RNN_na0, labelsval_gen_na, x_test_RNN_na0, y_test_gen_na)\n",
    "R2P0_gen_history, R2P0_gen_score_dict = train_and_evaluate_model(R2P_gen, x_train_RNN_na0, y_train_gen_na, dataval_RNN_na0, labelsval_gen_na, x_test_RNN_na0, y_test_gen_na)\n",
    "\n",
    "BiLSTM1_gen_history, BiLSTM1_gen_score_dict = train_and_evaluate_model(BiLSTM_gen, x_train_RNN_na1, y_train_gen_na, dataval_RNN_na1, labelsval_gen_na, x_test_RNN_na1, y_test_gen_na)\n",
    "ConvBiLSTM1_gen_history, ConvBiLSTM1_gen_score_dict = train_and_evaluate_model(ConvBiLSTM_gen, x_train_RNN_na1, y_train_gen_na, dataval_RNN_na1, labelsval_gen_na, x_test_RNN_na1, y_test_gen_na)\n",
    "R2P1_gen_history, R2P1_gen_score_dict = train_and_evaluate_model(R2P_gen, x_train_RNN_na1, y_train_gen_na, dataval_RNN_na1, labelsval_gen_na, x_test_RNN_na1, y_test_gen_na)\n",
    "\n",
    "# running the models at family and species level with the mutation rate adjusted one-hot-encoding\n",
    "BiLSTM1_fam_history, BiLSTM1_fam_score_dict = train_and_evaluate_model(BiLSTM_fam, x_train_RNN_na1, y_train_fam_na, dataval_RNN_na1, labelsval_fam_na, x_test_RNN_na1, y_test_fam_na)\n",
    "ConvBiLSTM1_fam_history, ConvBiLSTM1_fam_score_dict = train_and_evaluate_model(ConvBiLSTM_fam, x_train_RNN_na1, y_train_fam_na, dataval_RNN_na1, labelsval_fam_na, x_test_RNN_na1, y_test_fam_na)\n",
    "R2P1_fam_history, R2P1_fam_score_dict = train_and_evaluate_model(R2P_fam, x_train_RNN_na1, y_train_fam_na, dataval_RNN_na1, labelsval_fam_na, x_test_RNN_na1, y_test_fam_na)\n",
    "\n",
    "BiLSTM1_spe_history, BiLSTM1_spe_score_dict = train_and_evaluate_model(BiLSTM_spe, x_train_RNN_na1, y_train_spe_na, dataval_RNN_na1, labelsval_spe_na, x_test_RNN_na1, y_test_spe_na)\n",
    "ConvBiLSTM1_spe_history, ConvBiLSTM1_spe_score_dict = train_and_evaluate_model(ConvBiLSTM_spe, x_train_RNN_na1, y_train_spe_na, dataval_RNN_na1, labelsval_spe_na, x_test_RNN_na1, y_test_spe_na)\n",
    "R2P1_spe_history, R2P1_spe_score_dict = train_and_evaluate_model(R2P_spe, x_train_RNN_na1, y_train_spe_na, dataval_RNN_na1, labelsval_spe_na, x_test_RNN_na1, y_test_spe_na)\n",
    "\n",
    "# running the models at family level with the mutation rate adjusted one-hot-encoding on the augmented data\n",
    "BiLSTM1_gen_a_history, BiLSTM1_gen_a_score_dict = train_and_evaluate_model(BiLSTM_gen, x_train_RNN_a1, y_train_gen_a, dataval_RNN_a1, labelsval_gen_a, x_test_RNN_na1, y_test_gen_na)\n",
    "ConvBiLSTM1_gen_a_history, ConvBiLSTM1_gen_a_score_dict = train_and_evaluate_model(ConvBiLSTM_gen, x_train_RNN_a1, y_train_gen_a, dataval_RNN_a1, labelsval_gen_a, x_test_RNN_na1, y_test_gen_na)\n",
    "R2P1_gen_a_history, R2P1_gen_a_score_dict = train_and_evaluate_model(R2P_gen, x_train_RNN_a1, y_train_gen_a, dataval_RNN_a1, labelsval_gen_a, x_test_RNN_na1, y_test_gen_na)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2968f00109fd7d921f3cc346618df651342f4cc294a67a3de2d408bffb084936"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
